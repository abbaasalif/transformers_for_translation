{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer for translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZBAlmYui+r5FqXOEYNAkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbaasalif/trasnformers_for_translation/blob/main/Transformer_for_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVqZf9UzFj-R"
      },
      "source": [
        "# Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Qt4CmzFbww"
      },
      "source": [
        "import numpy as np\r\n",
        "import math\r\n",
        "import re\r\n",
        "import time\r\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKg2gMb8LwR"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjio5E0KB2I1"
      },
      "source": [
        "# Data preprocessing\r\n",
        "\r\n",
        "## Load the files\r\n",
        "\r\n",
        "- we import the files from our personal drive folder\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvfZDMNl8V9f",
        "outputId": "6e1a95de-af0c-405c-de02-ee01e9537ae9"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOPc9ew3FnAR"
      },
      "source": [
        "with open('/content/drive/MyDrive/transformers/P85-Non-Breaking-Prefix.en',\r\n",
        "            mode='r',\r\n",
        "          encoding='utf-8') as f:\r\n",
        "          non_breaking_prefix_en = f.read()\r\n",
        "with open('/content/drive/MyDrive/transformers/P85-Non-Breaking-Prefix.fr',\r\n",
        "            mode='r',\r\n",
        "          encoding='utf-8') as f:\r\n",
        "          non_breaking_prefix_fr = f.read()\r\n",
        "with open('/content/drive/MyDrive/transformers/europarl-v7.fr-en.en',\r\n",
        "            mode='r',\r\n",
        "          encoding='utf-8') as f:\r\n",
        "          europarl_en = f.read()\r\n",
        "with open('/content/drive/MyDrive/transformers/europarl-v7.fr-en.fr',\r\n",
        "            mode='r',\r\n",
        "          encoding='utf-8') as f:\r\n",
        "          europarl_fr = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMnFjszRHAbK"
      },
      "source": [
        "# Cleaning Data\r\n",
        "\r\n",
        "- Getting the non_breaing_prefixes as a clean list of words with a point at the end so it is easier to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2hY11O9G19w"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\r\n",
        "non_breaking_prefix_en = [\" \"+ pref +\".\" for pref in non_breaking_prefix_en]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL27dBzLHje6"
      },
      "source": [
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\r\n",
        "non_breaking_prefix_fr = [\" \"+ pref +\".\" for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SbI235UH5xX"
      },
      "source": [
        "corpus_en = europarl_en \r\n",
        "#Adding ### after non ending sentence points\r\n",
        "for prefix in non_breaking_prefix_en:\r\n",
        "  corpus_en = corpus_en.replace(prefix, prefix+\"$$$\")\r\n",
        "corpus_en = re.sub(r\"\\.(?=[0=9]|[a-z]|[A-Z])\",\".$$$\", corpus_en)\r\n",
        "#Remove ### markers\r\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\r\n",
        "# Clear muliple spaces\r\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\r\n",
        "corpus_en = corpus_en.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGqI10PPJReF"
      },
      "source": [
        "corpus_fr = europarl_fr \r\n",
        "for prefix in non_breaking_prefix_fr:\r\n",
        "  corpus_fr = corpus_fr.replace(prefix, prefix+\"$$$\")\r\n",
        "corpus_fr = re.sub(r\"\\.(?=[0=9]|[a-z]|[A-Z])\",\".$$$\", corpus_fr)\r\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\",'', corpus_fr)\r\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\r\n",
        "corpus_fr = corpus_fr.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGVap6uJmir"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJmNxnuJjh7"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    corpus_en, target_vocab_size=2**13)\r\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqtZw_LgKucq"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size+2\r\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size+2\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfj7jP7tsi4v",
        "outputId": "1ef241f8-c4e9-4659-f374-b395fb99b9ef"
      },
      "source": [
        "VOCAB_SIZE_EN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19lH8ZV3K4Zq"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] \r\n",
        "          for sentence in corpus_en]\r\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1] \r\n",
        "          for sentence in corpus_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Uj6EE3Lh_Y"
      },
      "source": [
        "# Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg4ljqJDMXaw"
      },
      "source": [
        "# Padding the Inputs & Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMKoRzSmMVPF"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, \r\n",
        "                                              padding=\"post\",\r\n",
        "                                              maxlen=MAX_LENGTH)\r\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, \r\n",
        "                                              padding=\"post\",\r\n",
        "                                              maxlen=MAX_LENGTH)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5T2hvvM64k"
      },
      "source": [
        "BATCH_SIZE= 64\r\n",
        "BUFFER_SIZE = 20000 # shuffling of dataset\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\r\n",
        "\r\n",
        "dataset = dataset.cache()\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rOYmKuCUKFR"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n0B2BeTUOmm"
      },
      "source": [
        "## Embedding\r\n",
        "\r\n",
        "Positional encoding formulas:\r\n",
        "\r\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\r\n",
        "\r\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRJyvyGDUV2N"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "    \r\n",
        "    def get_angles(self, pos, i, d_model):\r\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\r\n",
        "        return pos * angles\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        seq_length = inputs.shape.as_list()[-2]\r\n",
        "        d_model = inputs.shape.as_list()[-1]\r\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\r\n",
        "                                 np.arange(d_model)[np.newaxis, :],\r\n",
        "                                 d_model)\r\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\r\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\r\n",
        "        pos_encoding = angles[np.newaxis, ...]\r\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj"
      },
      "source": [
        "### Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sLhBhAYW0gB"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\r\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\r\n",
        "    \r\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\r\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\r\n",
        "    \r\n",
        "    if mask is not None:\r\n",
        "        scaled_product += (mask * -1e9)\r\n",
        "    \r\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\r\n",
        "    \r\n",
        "    return attention\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJrSNnT-X0CU"
      },
      "source": [
        "## Multi-headed attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OJm49aQW5-W"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, nb_proj):\r\n",
        "        super(MultiHeadAttention, self).__init__()\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        \r\n",
        "    def build(self, input_shape):\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "        assert self.d_model % self.nb_proj == 0\r\n",
        "        \r\n",
        "        self.d_proj = self.d_model // self.nb_proj\r\n",
        "        \r\n",
        "        self.query_lin = layers.Dense(units=self.d_model)\r\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\r\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\r\n",
        "        \r\n",
        "        self.final_lin = layers.Dense(units=self.d_model)\r\n",
        "        \r\n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\r\n",
        "        shape = (batch_size,\r\n",
        "                 -1,\r\n",
        "                 self.nb_proj,\r\n",
        "                 self.d_proj)\r\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\r\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\r\n",
        "    \r\n",
        "    def call(self, queries, keys, values, mask):\r\n",
        "        batch_size = tf.shape(queries)[0]\r\n",
        "        \r\n",
        "        queries = self.query_lin(queries)\r\n",
        "        keys = self.key_lin(keys)\r\n",
        "        values = self.value_lin(values)\r\n",
        "        \r\n",
        "        queries = self.split_proj(queries, batch_size)\r\n",
        "        keys = self.split_proj(keys, batch_size)\r\n",
        "        values = self.split_proj(values, batch_size)\r\n",
        "        \r\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\r\n",
        "        \r\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\r\n",
        "        \r\n",
        "        concat_attention = tf.reshape(attention,\r\n",
        "                                      shape=(batch_size, -1, self.d_model))\r\n",
        "        \r\n",
        "        outputs = self.final_lin(concat_attention)\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abCY9sVVbWll"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y90JQEa3bVAw"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        self.FFN_units = FFN_units\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        self.dropout_rate = dropout_rate\r\n",
        "    \r\n",
        "    def build(self, input_shape):\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "        \r\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\r\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\r\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "    def call(self, inputs, mask, training):\r\n",
        "        attention = self.multi_head_attention(inputs,\r\n",
        "                                              inputs,\r\n",
        "                                              inputs,\r\n",
        "                                              mask)\r\n",
        "        attention = self.dropout_1(attention, training=training)\r\n",
        "        attention = self.norm_1(attention + inputs)\r\n",
        "        \r\n",
        "        outputs = self.dense_1(attention)\r\n",
        "        outputs = self.dense_2(outputs)\r\n",
        "        outputs = self.dropout_2(outputs, training=training)\r\n",
        "        outputs = self.norm_2(outputs + attention)\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBudipOse5kp"
      },
      "source": [
        "class Encoder(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 nb_layers,\r\n",
        "                 FFN_units,\r\n",
        "                 nb_proj,\r\n",
        "                 dropout_rate,\r\n",
        "                 vocab_size,\r\n",
        "                 d_model,\r\n",
        "                 name=\"encoder\"):\r\n",
        "        super(Encoder, self).__init__(name=name)\r\n",
        "        self.nb_layers = nb_layers\r\n",
        "        self.d_model = d_model\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\r\n",
        "        self.pos_encoding = PositionalEncoding()\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\r\n",
        "                                        nb_proj,\r\n",
        "                                        dropout_rate) \r\n",
        "                           for _ in range(nb_layers)]\r\n",
        "    \r\n",
        "    def call(self, inputs, mask, training):\r\n",
        "        outputs = self.embedding(inputs)\r\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "        outputs = self.pos_encoding(outputs)\r\n",
        "        outputs = self.dropout(outputs, training)\r\n",
        "        \r\n",
        "        for i in range(self.nb_layers):\r\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7rfS7WHgzBG"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io6FXQR0gz-_"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n",
        "        super(DecoderLayer, self).__init__()\r\n",
        "        self.FFN_units = FFN_units\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        self.dropout_rate = dropout_rate\r\n",
        "    \r\n",
        "    def build(self, input_shape):\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "        \r\n",
        "        # Self multi head attention\r\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        # Multi head attention combined with encoder output\r\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        # Feed foward\r\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\r\n",
        "                                    activation=\"relu\")\r\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\r\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\r\n",
        "        attention = self.multi_head_attention_1(inputs,\r\n",
        "                                                inputs,\r\n",
        "                                                inputs,\r\n",
        "                                                mask_1)\r\n",
        "        attention = self.dropout_1(attention, training)\r\n",
        "        attention = self.norm_1(attention + inputs)\r\n",
        "        \r\n",
        "        attention_2 = self.multi_head_attention_2(attention,\r\n",
        "                                                  enc_outputs,\r\n",
        "                                                  enc_outputs,\r\n",
        "                                                  mask_2)\r\n",
        "        attention_2 = self.dropout_2(attention_2, training)\r\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\r\n",
        "        \r\n",
        "        outputs = self.dense_1(attention_2)\r\n",
        "        outputs = self.dense_2(outputs)\r\n",
        "        outputs = self.dropout_3(outputs, training)\r\n",
        "        outputs = self.norm_3(outputs + attention_2)\r\n",
        "        \r\n",
        "        return outputs\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qThGDC8Qed7a"
      },
      "source": [
        "class Decoder(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 nb_layers,\r\n",
        "                 FFN_units,\r\n",
        "                 nb_proj,\r\n",
        "                 dropout_rate,\r\n",
        "                 vocab_size,\r\n",
        "                 d_model,\r\n",
        "                 name=\"decoder\"):\r\n",
        "        super(Decoder, self).__init__(name=name)\r\n",
        "        self.d_model = d_model\r\n",
        "        self.nb_layers = nb_layers\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\r\n",
        "        self.pos_encoding = PositionalEncoding()\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        \r\n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\r\n",
        "                                        nb_proj,\r\n",
        "                                        dropout_rate) \r\n",
        "                           for i in range(nb_layers)]\r\n",
        "    \r\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\r\n",
        "        outputs = self.embedding(inputs)\r\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "        outputs = self.pos_encoding(outputs)\r\n",
        "        outputs = self.dropout(outputs, training)\r\n",
        "        \r\n",
        "        for i in range(self.nb_layers):\r\n",
        "            outputs = self.dec_layers[i](outputs,\r\n",
        "                                         enc_outputs,\r\n",
        "                                         mask_1,\r\n",
        "                                         mask_2,\r\n",
        "                                         training)\r\n",
        "\r\n",
        "        return outputs\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns-Re9x_gF0F"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNtMpbMgIaw"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 vocab_size_enc,\r\n",
        "                 vocab_size_dec,\r\n",
        "                 d_model,\r\n",
        "                 nb_layers,\r\n",
        "                 FFN_units,\r\n",
        "                 nb_proj,\r\n",
        "                 dropout_rate,\r\n",
        "                 name=\"transformer\"):\r\n",
        "        super(Transformer, self).__init__(name=name)\r\n",
        "        \r\n",
        "        self.encoder = Encoder(nb_layers,\r\n",
        "                               FFN_units,\r\n",
        "                               nb_proj,\r\n",
        "                               dropout_rate,\r\n",
        "                               vocab_size_enc,\r\n",
        "                               d_model)\r\n",
        "        self.decoder = Decoder(nb_layers,\r\n",
        "                               FFN_units,\r\n",
        "                               nb_proj,\r\n",
        "                               dropout_rate,\r\n",
        "                               vocab_size_dec,\r\n",
        "                               d_model)\r\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\r\n",
        "    \r\n",
        "    def create_padding_mask(self, seq):\r\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "    def create_look_ahead_mask(self, seq):\r\n",
        "        seq_len = tf.shape(seq)[1]\r\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\r\n",
        "        return look_ahead_mask\r\n",
        "    \r\n",
        "    def call(self, enc_inputs, dec_inputs, training):\r\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\r\n",
        "        dec_mask_1 = tf.maximum(\r\n",
        "            self.create_padding_mask(dec_inputs),\r\n",
        "            self.create_look_ahead_mask(dec_inputs)\r\n",
        "        )\r\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\r\n",
        "        \r\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\r\n",
        "        dec_outputs = self.decoder(dec_inputs,\r\n",
        "                                   enc_outputs,\r\n",
        "                                   dec_mask_1,\r\n",
        "                                   dec_mask_2,\r\n",
        "                                   training)\r\n",
        "        \r\n",
        "        outputs = self.last_linear(dec_outputs)\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9IiNZnrl73W"
      },
      "source": [
        "# Training\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnx_FdFTl_SN"
      },
      "source": [
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "# Hyper-parameters\r\n",
        "D_MODEL = 128 # 512\r\n",
        "NB_LAYERS = 4 # 6\r\n",
        "FFN_UNITS = 512 # 2048\r\n",
        "NB_PROJ = 8 # 8\r\n",
        "DROPOUT_RATE = 0.1 # 0.1\r\n",
        "\r\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\r\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\r\n",
        "                          d_model=D_MODEL,\r\n",
        "                          nb_layers=NB_LAYERS,\r\n",
        "                          FFN_units=FFN_UNITS,\r\n",
        "                          nb_proj=NB_PROJ,\r\n",
        "                          dropout_rate=DROPOUT_RATE)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HnEsyhenVTf"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\r\n",
        "                                                            reduction = \"none\"\r\n",
        "                                                            )\r\n",
        "\r\n",
        "def loss_function(target, pred):\r\n",
        "  mask = tf.math.logical_not(tf.equal(target,0))\r\n",
        "  loss_ = loss_object(target, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\r\n",
        "\r\n",
        "  loss_ *= mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss_)\r\n",
        "\r\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\r\n",
        "\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AshkniaPoQzH"
      },
      "source": [
        "class CustomSchedule(tf.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\r\n",
        "\r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\r\n",
        "\r\n",
        "learning_rate = CustomSchedule(D_MODEL)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(\r\n",
        "    learning_rate,\r\n",
        "    beta_1 = 0.9,\r\n",
        "    beta_2 = 0.98,\r\n",
        "    epsilon = 1e-9\r\n",
        ")\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0pgQ1p0wclm"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/transformers/ckpt\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(transformer = transformer,\r\n",
        "                           optimizer = optimizer)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "  print(\"Latest Checkpoint restored!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdry-z7Gw6m9",
        "outputId": "a122faa8-a90f-4a9c-b45c-833a13ebef85"
      },
      "source": [
        "EPOCHS = 10\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\r\n",
        "    start = time.time()\r\n",
        "    \r\n",
        "    train_loss.reset_states()\r\n",
        "    train_accuracy.reset_states()\r\n",
        "    \r\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\r\n",
        "        dec_inputs = targets[:, :-1]\r\n",
        "        dec_outputs_real = targets[:, 1:]\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\r\n",
        "            loss = loss_function(dec_outputs_real, predictions)\r\n",
        "        \r\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\r\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "        \r\n",
        "        train_loss(loss)\r\n",
        "        train_accuracy(dec_outputs_real, predictions)\r\n",
        "        \r\n",
        "        if batch % 50 == 0:\r\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\r\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\r\n",
        "            \r\n",
        "    ckpt_save_path = ckpt_manager.save()\r\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\r\n",
        "                                                        ckpt_save_path))\r\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.4556 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.2172 Accuracy 0.0039\n",
            "Epoch 1 Batch 100 Loss 6.1713 Accuracy 0.0244\n",
            "Epoch 1 Batch 150 Loss 6.1151 Accuracy 0.0328\n",
            "Epoch 1 Batch 200 Loss 6.0311 Accuracy 0.0376\n",
            "Epoch 1 Batch 250 Loss 5.9397 Accuracy 0.0457\n",
            "Epoch 1 Batch 300 Loss 5.8240 Accuracy 0.0534\n",
            "Epoch 1 Batch 350 Loss 5.6949 Accuracy 0.0590\n",
            "Epoch 1 Batch 400 Loss 5.5630 Accuracy 0.0633\n",
            "Epoch 1 Batch 450 Loss 5.4411 Accuracy 0.0668\n",
            "Epoch 1 Batch 500 Loss 5.3266 Accuracy 0.0712\n",
            "Epoch 1 Batch 550 Loss 5.2196 Accuracy 0.0765\n",
            "Epoch 1 Batch 600 Loss 5.1200 Accuracy 0.0818\n",
            "Epoch 1 Batch 650 Loss 5.0224 Accuracy 0.0870\n",
            "Epoch 1 Batch 700 Loss 4.9353 Accuracy 0.0920\n",
            "Epoch 1 Batch 750 Loss 4.8500 Accuracy 0.0970\n",
            "Epoch 1 Batch 800 Loss 4.7715 Accuracy 0.1020\n",
            "Epoch 1 Batch 850 Loss 4.6937 Accuracy 0.1069\n",
            "Epoch 1 Batch 900 Loss 4.6186 Accuracy 0.1115\n",
            "Epoch 1 Batch 950 Loss 4.5481 Accuracy 0.1160\n",
            "Epoch 1 Batch 1000 Loss 4.4841 Accuracy 0.1204\n",
            "Epoch 1 Batch 1050 Loss 4.4225 Accuracy 0.1244\n",
            "Epoch 1 Batch 1100 Loss 4.3649 Accuracy 0.1283\n",
            "Epoch 1 Batch 1150 Loss 4.3116 Accuracy 0.1318\n",
            "Epoch 1 Batch 1200 Loss 4.2622 Accuracy 0.1351\n",
            "Epoch 1 Batch 1250 Loss 4.2138 Accuracy 0.1385\n",
            "Epoch 1 Batch 1300 Loss 4.1666 Accuracy 0.1415\n",
            "Epoch 1 Batch 1350 Loss 4.1236 Accuracy 0.1447\n",
            "Epoch 1 Batch 1400 Loss 4.0806 Accuracy 0.1478\n",
            "Epoch 1 Batch 1450 Loss 4.0397 Accuracy 0.1509\n",
            "Epoch 1 Batch 1500 Loss 4.0000 Accuracy 0.1540\n",
            "Epoch 1 Batch 1550 Loss 3.9626 Accuracy 0.1570\n",
            "Epoch 1 Batch 1600 Loss 3.9273 Accuracy 0.1598\n",
            "Epoch 1 Batch 1650 Loss 3.8928 Accuracy 0.1627\n",
            "Epoch 1 Batch 1700 Loss 3.8607 Accuracy 0.1653\n",
            "Epoch 1 Batch 1750 Loss 3.8276 Accuracy 0.1680\n",
            "Epoch 1 Batch 1800 Loss 3.7971 Accuracy 0.1706\n",
            "Epoch 1 Batch 1850 Loss 3.7673 Accuracy 0.1731\n",
            "Epoch 1 Batch 1900 Loss 3.7377 Accuracy 0.1756\n",
            "Epoch 1 Batch 1950 Loss 3.7105 Accuracy 0.1780\n",
            "Epoch 1 Batch 2000 Loss 3.6832 Accuracy 0.1802\n",
            "Epoch 1 Batch 2050 Loss 3.6564 Accuracy 0.1823\n",
            "Epoch 1 Batch 2100 Loss 3.6293 Accuracy 0.1843\n",
            "Epoch 1 Batch 2150 Loss 3.6033 Accuracy 0.1861\n",
            "Epoch 1 Batch 2200 Loss 3.5779 Accuracy 0.1881\n",
            "Epoch 1 Batch 2250 Loss 3.5520 Accuracy 0.1898\n",
            "Epoch 1 Batch 2300 Loss 3.5275 Accuracy 0.1916\n",
            "Epoch 1 Batch 2350 Loss 3.5039 Accuracy 0.1934\n",
            "Epoch 1 Batch 2400 Loss 3.4800 Accuracy 0.1953\n",
            "Epoch 1 Batch 2450 Loss 3.4565 Accuracy 0.1970\n",
            "Epoch 1 Batch 2500 Loss 3.4330 Accuracy 0.1989\n",
            "Epoch 1 Batch 2550 Loss 3.4099 Accuracy 0.2008\n",
            "Epoch 1 Batch 2600 Loss 3.3878 Accuracy 0.2027\n",
            "Epoch 1 Batch 2650 Loss 3.3658 Accuracy 0.2046\n",
            "Epoch 1 Batch 2700 Loss 3.3448 Accuracy 0.2065\n",
            "Epoch 1 Batch 2750 Loss 3.3242 Accuracy 0.2084\n",
            "Epoch 1 Batch 2800 Loss 3.3037 Accuracy 0.2102\n",
            "Epoch 1 Batch 2850 Loss 3.2837 Accuracy 0.2121\n",
            "Epoch 1 Batch 2900 Loss 3.2641 Accuracy 0.2139\n",
            "Epoch 1 Batch 2950 Loss 3.2444 Accuracy 0.2157\n",
            "Epoch 1 Batch 3000 Loss 3.2257 Accuracy 0.2175\n",
            "Epoch 1 Batch 3050 Loss 3.2073 Accuracy 0.2193\n",
            "Epoch 1 Batch 3100 Loss 3.1890 Accuracy 0.2210\n",
            "Epoch 1 Batch 3150 Loss 3.1708 Accuracy 0.2227\n",
            "Epoch 1 Batch 3200 Loss 3.1529 Accuracy 0.2244\n",
            "Epoch 1 Batch 3250 Loss 3.1348 Accuracy 0.2262\n",
            "Epoch 1 Batch 3300 Loss 3.1175 Accuracy 0.2279\n",
            "Epoch 1 Batch 3350 Loss 3.1000 Accuracy 0.2297\n",
            "Epoch 1 Batch 3400 Loss 3.0829 Accuracy 0.2314\n",
            "Epoch 1 Batch 3450 Loss 3.0662 Accuracy 0.2331\n",
            "Epoch 1 Batch 3500 Loss 3.0493 Accuracy 0.2348\n",
            "Epoch 1 Batch 3550 Loss 3.0336 Accuracy 0.2365\n",
            "Epoch 1 Batch 3600 Loss 3.0176 Accuracy 0.2382\n",
            "Epoch 1 Batch 3650 Loss 3.0019 Accuracy 0.2399\n",
            "Epoch 1 Batch 3700 Loss 2.9859 Accuracy 0.2416\n",
            "Epoch 1 Batch 3750 Loss 2.9706 Accuracy 0.2433\n",
            "Epoch 1 Batch 3800 Loss 2.9564 Accuracy 0.2450\n",
            "Epoch 1 Batch 3850 Loss 2.9418 Accuracy 0.2467\n",
            "Epoch 1 Batch 3900 Loss 2.9271 Accuracy 0.2483\n",
            "Epoch 1 Batch 3950 Loss 2.9128 Accuracy 0.2499\n",
            "Epoch 1 Batch 4000 Loss 2.8991 Accuracy 0.2515\n",
            "Epoch 1 Batch 4050 Loss 2.8854 Accuracy 0.2531\n",
            "Epoch 1 Batch 4100 Loss 2.8717 Accuracy 0.2546\n",
            "Epoch 1 Batch 4150 Loss 2.8590 Accuracy 0.2560\n",
            "Epoch 1 Batch 4200 Loss 2.8469 Accuracy 0.2573\n",
            "Epoch 1 Batch 4250 Loss 2.8355 Accuracy 0.2586\n",
            "Epoch 1 Batch 4300 Loss 2.8244 Accuracy 0.2599\n",
            "Epoch 1 Batch 4350 Loss 2.8139 Accuracy 0.2610\n",
            "Epoch 1 Batch 4400 Loss 2.8030 Accuracy 0.2622\n",
            "Epoch 1 Batch 4450 Loss 2.7926 Accuracy 0.2633\n",
            "Epoch 1 Batch 4500 Loss 2.7822 Accuracy 0.2645\n",
            "Epoch 1 Batch 4550 Loss 2.7721 Accuracy 0.2655\n",
            "Epoch 1 Batch 4600 Loss 2.7622 Accuracy 0.2666\n",
            "Epoch 1 Batch 4650 Loss 2.7523 Accuracy 0.2677\n",
            "Epoch 1 Batch 4700 Loss 2.7425 Accuracy 0.2688\n",
            "Epoch 1 Batch 4750 Loss 2.7331 Accuracy 0.2698\n",
            "Epoch 1 Batch 4800 Loss 2.7236 Accuracy 0.2709\n",
            "Epoch 1 Batch 4850 Loss 2.7141 Accuracy 0.2719\n",
            "Epoch 1 Batch 4900 Loss 2.7050 Accuracy 0.2729\n",
            "Epoch 1 Batch 4950 Loss 2.6961 Accuracy 0.2739\n",
            "Epoch 1 Batch 5000 Loss 2.6871 Accuracy 0.2750\n",
            "Epoch 1 Batch 5050 Loss 2.6783 Accuracy 0.2759\n",
            "Epoch 1 Batch 5100 Loss 2.6692 Accuracy 0.2768\n",
            "Epoch 1 Batch 5150 Loss 2.6605 Accuracy 0.2777\n",
            "Epoch 1 Batch 5200 Loss 2.6521 Accuracy 0.2786\n",
            "Epoch 1 Batch 5250 Loss 2.6434 Accuracy 0.2795\n",
            "Epoch 1 Batch 5300 Loss 2.6347 Accuracy 0.2803\n",
            "Epoch 1 Batch 5350 Loss 2.6260 Accuracy 0.2811\n",
            "Epoch 1 Batch 5400 Loss 2.6177 Accuracy 0.2819\n",
            "Epoch 1 Batch 5450 Loss 2.6092 Accuracy 0.2827\n",
            "Epoch 1 Batch 5500 Loss 2.6009 Accuracy 0.2836\n",
            "Epoch 1 Batch 5550 Loss 2.5925 Accuracy 0.2844\n",
            "Epoch 1 Batch 5600 Loss 2.5848 Accuracy 0.2852\n",
            "Epoch 1 Batch 5650 Loss 2.5767 Accuracy 0.2861\n",
            "Epoch 1 Batch 5700 Loss 2.5688 Accuracy 0.2869\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/transformers/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 1496.498385667801 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7793 Accuracy 0.3265\n",
            "Epoch 2 Batch 50 Loss 1.6944 Accuracy 0.3796\n",
            "Epoch 2 Batch 100 Loss 1.7066 Accuracy 0.3828\n",
            "Epoch 2 Batch 150 Loss 1.7021 Accuracy 0.3840\n",
            "Epoch 2 Batch 200 Loss 1.6943 Accuracy 0.3855\n",
            "Epoch 2 Batch 250 Loss 1.6853 Accuracy 0.3873\n",
            "Epoch 2 Batch 300 Loss 1.6805 Accuracy 0.3873\n",
            "Epoch 2 Batch 350 Loss 1.6748 Accuracy 0.3877\n",
            "Epoch 2 Batch 400 Loss 1.6677 Accuracy 0.3882\n",
            "Epoch 2 Batch 450 Loss 1.6589 Accuracy 0.3888\n",
            "Epoch 2 Batch 500 Loss 1.6516 Accuracy 0.3893\n",
            "Epoch 2 Batch 550 Loss 1.6482 Accuracy 0.3898\n",
            "Epoch 2 Batch 600 Loss 1.6477 Accuracy 0.3903\n",
            "Epoch 2 Batch 650 Loss 1.6441 Accuracy 0.3906\n",
            "Epoch 2 Batch 700 Loss 1.6395 Accuracy 0.3915\n",
            "Epoch 2 Batch 750 Loss 1.6362 Accuracy 0.3920\n",
            "Epoch 2 Batch 800 Loss 1.6328 Accuracy 0.3926\n",
            "Epoch 2 Batch 850 Loss 1.6305 Accuracy 0.3928\n",
            "Epoch 2 Batch 900 Loss 1.6272 Accuracy 0.3929\n",
            "Epoch 2 Batch 950 Loss 1.6235 Accuracy 0.3929\n",
            "Epoch 2 Batch 1000 Loss 1.6191 Accuracy 0.3934\n",
            "Epoch 2 Batch 1050 Loss 1.6154 Accuracy 0.3936\n",
            "Epoch 2 Batch 1100 Loss 1.6142 Accuracy 0.3939\n",
            "Epoch 2 Batch 1150 Loss 1.6116 Accuracy 0.3940\n",
            "Epoch 2 Batch 1200 Loss 1.6079 Accuracy 0.3944\n",
            "Epoch 2 Batch 1250 Loss 1.6049 Accuracy 0.3946\n",
            "Epoch 2 Batch 1300 Loss 1.6030 Accuracy 0.3952\n",
            "Epoch 2 Batch 1350 Loss 1.5992 Accuracy 0.3960\n",
            "Epoch 2 Batch 1400 Loss 1.5950 Accuracy 0.3970\n",
            "Epoch 2 Batch 1450 Loss 1.5923 Accuracy 0.3979\n",
            "Epoch 2 Batch 1500 Loss 1.5867 Accuracy 0.3988\n",
            "Epoch 2 Batch 1550 Loss 1.5819 Accuracy 0.3998\n",
            "Epoch 2 Batch 1600 Loss 1.5784 Accuracy 0.4008\n",
            "Epoch 2 Batch 1650 Loss 1.5741 Accuracy 0.4018\n",
            "Epoch 2 Batch 1700 Loss 1.5703 Accuracy 0.4027\n",
            "Epoch 2 Batch 1750 Loss 1.5671 Accuracy 0.4036\n",
            "Epoch 2 Batch 1800 Loss 1.5637 Accuracy 0.4046\n",
            "Epoch 2 Batch 1850 Loss 1.5594 Accuracy 0.4056\n",
            "Epoch 2 Batch 1900 Loss 1.5559 Accuracy 0.4066\n",
            "Epoch 2 Batch 1950 Loss 1.5522 Accuracy 0.4075\n",
            "Epoch 2 Batch 2000 Loss 1.5487 Accuracy 0.4083\n",
            "Epoch 2 Batch 2050 Loss 1.5448 Accuracy 0.4091\n",
            "Epoch 2 Batch 2100 Loss 1.5405 Accuracy 0.4096\n",
            "Epoch 2 Batch 2150 Loss 1.5363 Accuracy 0.4101\n",
            "Epoch 2 Batch 2200 Loss 1.5311 Accuracy 0.4106\n",
            "Epoch 2 Batch 2250 Loss 1.5264 Accuracy 0.4110\n",
            "Epoch 2 Batch 2300 Loss 1.5212 Accuracy 0.4114\n",
            "Epoch 2 Batch 2350 Loss 1.5164 Accuracy 0.4119\n",
            "Epoch 2 Batch 2400 Loss 1.5124 Accuracy 0.4123\n",
            "Epoch 2 Batch 2450 Loss 1.5078 Accuracy 0.4128\n",
            "Epoch 2 Batch 2500 Loss 1.5035 Accuracy 0.4135\n",
            "Epoch 2 Batch 2550 Loss 1.4984 Accuracy 0.4141\n",
            "Epoch 2 Batch 2600 Loss 1.4942 Accuracy 0.4147\n",
            "Epoch 2 Batch 2650 Loss 1.4901 Accuracy 0.4153\n",
            "Epoch 2 Batch 2700 Loss 1.4857 Accuracy 0.4160\n",
            "Epoch 2 Batch 2750 Loss 1.4809 Accuracy 0.4166\n",
            "Epoch 2 Batch 2800 Loss 1.4767 Accuracy 0.4171\n",
            "Epoch 2 Batch 2850 Loss 1.4726 Accuracy 0.4176\n",
            "Epoch 2 Batch 2900 Loss 1.4687 Accuracy 0.4181\n",
            "Epoch 2 Batch 2950 Loss 1.4649 Accuracy 0.4186\n",
            "Epoch 2 Batch 3000 Loss 1.4608 Accuracy 0.4192\n",
            "Epoch 2 Batch 3050 Loss 1.4571 Accuracy 0.4197\n",
            "Epoch 2 Batch 3100 Loss 1.4535 Accuracy 0.4202\n",
            "Epoch 2 Batch 3150 Loss 1.4496 Accuracy 0.4207\n",
            "Epoch 2 Batch 3200 Loss 1.4461 Accuracy 0.4213\n",
            "Epoch 2 Batch 3250 Loss 1.4424 Accuracy 0.4217\n",
            "Epoch 2 Batch 3300 Loss 1.4385 Accuracy 0.4223\n",
            "Epoch 2 Batch 3350 Loss 1.4346 Accuracy 0.4227\n",
            "Epoch 2 Batch 3400 Loss 1.4313 Accuracy 0.4232\n",
            "Epoch 2 Batch 3450 Loss 1.4274 Accuracy 0.4238\n",
            "Epoch 2 Batch 3500 Loss 1.4239 Accuracy 0.4243\n",
            "Epoch 2 Batch 3550 Loss 1.4205 Accuracy 0.4249\n",
            "Epoch 2 Batch 3600 Loss 1.4171 Accuracy 0.4254\n",
            "Epoch 2 Batch 3650 Loss 1.4138 Accuracy 0.4260\n",
            "Epoch 2 Batch 3700 Loss 1.4108 Accuracy 0.4266\n",
            "Epoch 2 Batch 3750 Loss 1.4078 Accuracy 0.4272\n",
            "Epoch 2 Batch 3800 Loss 1.4047 Accuracy 0.4277\n",
            "Epoch 2 Batch 3850 Loss 1.4015 Accuracy 0.4282\n",
            "Epoch 2 Batch 3900 Loss 1.3982 Accuracy 0.4288\n",
            "Epoch 2 Batch 3950 Loss 1.3953 Accuracy 0.4293\n",
            "Epoch 2 Batch 4000 Loss 1.3922 Accuracy 0.4299\n",
            "Epoch 2 Batch 4050 Loss 1.3893 Accuracy 0.4305\n",
            "Epoch 2 Batch 4100 Loss 1.3868 Accuracy 0.4309\n",
            "Epoch 2 Batch 4150 Loss 1.3850 Accuracy 0.4312\n",
            "Epoch 2 Batch 4200 Loss 1.3835 Accuracy 0.4314\n",
            "Epoch 2 Batch 4250 Loss 1.3824 Accuracy 0.4316\n",
            "Epoch 2 Batch 4300 Loss 1.3817 Accuracy 0.4318\n",
            "Epoch 2 Batch 4350 Loss 1.3811 Accuracy 0.4319\n",
            "Epoch 2 Batch 4400 Loss 1.3807 Accuracy 0.4319\n",
            "Epoch 2 Batch 4450 Loss 1.3806 Accuracy 0.4320\n",
            "Epoch 2 Batch 4500 Loss 1.3802 Accuracy 0.4321\n",
            "Epoch 2 Batch 4550 Loss 1.3801 Accuracy 0.4320\n",
            "Epoch 2 Batch 4600 Loss 1.3804 Accuracy 0.4321\n",
            "Epoch 2 Batch 4650 Loss 1.3804 Accuracy 0.4321\n",
            "Epoch 2 Batch 4700 Loss 1.3808 Accuracy 0.4321\n",
            "Epoch 2 Batch 4750 Loss 1.3808 Accuracy 0.4321\n",
            "Epoch 2 Batch 4800 Loss 1.3807 Accuracy 0.4322\n",
            "Epoch 2 Batch 4850 Loss 1.3808 Accuracy 0.4322\n",
            "Epoch 2 Batch 4900 Loss 1.3807 Accuracy 0.4322\n",
            "Epoch 2 Batch 4950 Loss 1.3808 Accuracy 0.4322\n",
            "Epoch 2 Batch 5000 Loss 1.3807 Accuracy 0.4322\n",
            "Epoch 2 Batch 5050 Loss 1.3808 Accuracy 0.4322\n",
            "Epoch 2 Batch 5100 Loss 1.3808 Accuracy 0.4322\n",
            "Epoch 2 Batch 5150 Loss 1.3808 Accuracy 0.4321\n",
            "Epoch 2 Batch 5200 Loss 1.3806 Accuracy 0.4321\n",
            "Epoch 2 Batch 5250 Loss 1.3805 Accuracy 0.4321\n",
            "Epoch 2 Batch 5300 Loss 1.3802 Accuracy 0.4320\n",
            "Epoch 2 Batch 5350 Loss 1.3800 Accuracy 0.4319\n",
            "Epoch 2 Batch 5400 Loss 1.3797 Accuracy 0.4318\n",
            "Epoch 2 Batch 5450 Loss 1.3796 Accuracy 0.4317\n",
            "Epoch 2 Batch 5500 Loss 1.3792 Accuracy 0.4316\n",
            "Epoch 2 Batch 5550 Loss 1.3789 Accuracy 0.4316\n",
            "Epoch 2 Batch 5600 Loss 1.3785 Accuracy 0.4316\n",
            "Epoch 2 Batch 5650 Loss 1.3780 Accuracy 0.4315\n",
            "Epoch 2 Batch 5700 Loss 1.3776 Accuracy 0.4315\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/transformers/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 1497.8662633895874 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.3111 Accuracy 0.4285\n",
            "Epoch 3 Batch 50 Loss 1.3697 Accuracy 0.4329\n",
            "Epoch 3 Batch 100 Loss 1.3564 Accuracy 0.4356\n",
            "Epoch 3 Batch 150 Loss 1.3475 Accuracy 0.4346\n",
            "Epoch 3 Batch 200 Loss 1.3437 Accuracy 0.4357\n",
            "Epoch 3 Batch 250 Loss 1.3463 Accuracy 0.4363\n",
            "Epoch 3 Batch 300 Loss 1.3429 Accuracy 0.4360\n",
            "Epoch 3 Batch 350 Loss 1.3418 Accuracy 0.4358\n",
            "Epoch 3 Batch 400 Loss 1.3377 Accuracy 0.4360\n",
            "Epoch 3 Batch 450 Loss 1.3342 Accuracy 0.4360\n",
            "Epoch 3 Batch 500 Loss 1.3304 Accuracy 0.4366\n",
            "Epoch 3 Batch 550 Loss 1.3282 Accuracy 0.4369\n",
            "Epoch 3 Batch 600 Loss 1.3261 Accuracy 0.4370\n",
            "Epoch 3 Batch 650 Loss 1.3236 Accuracy 0.4374\n",
            "Epoch 3 Batch 700 Loss 1.3216 Accuracy 0.4378\n",
            "Epoch 3 Batch 750 Loss 1.3175 Accuracy 0.4386\n",
            "Epoch 3 Batch 800 Loss 1.3163 Accuracy 0.4391\n",
            "Epoch 3 Batch 850 Loss 1.3152 Accuracy 0.4391\n",
            "Epoch 3 Batch 900 Loss 1.3148 Accuracy 0.4390\n",
            "Epoch 3 Batch 950 Loss 1.3127 Accuracy 0.4389\n",
            "Epoch 3 Batch 1000 Loss 1.3104 Accuracy 0.4390\n",
            "Epoch 3 Batch 1050 Loss 1.3082 Accuracy 0.4389\n",
            "Epoch 3 Batch 1100 Loss 1.3059 Accuracy 0.4389\n",
            "Epoch 3 Batch 1150 Loss 1.3041 Accuracy 0.4389\n",
            "Epoch 3 Batch 1200 Loss 1.3027 Accuracy 0.4392\n",
            "Epoch 3 Batch 1250 Loss 1.3005 Accuracy 0.4396\n",
            "Epoch 3 Batch 1300 Loss 1.2996 Accuracy 0.4401\n",
            "Epoch 3 Batch 1350 Loss 1.2966 Accuracy 0.4408\n",
            "Epoch 3 Batch 1400 Loss 1.2937 Accuracy 0.4417\n",
            "Epoch 3 Batch 1450 Loss 1.2912 Accuracy 0.4425\n",
            "Epoch 3 Batch 1500 Loss 1.2882 Accuracy 0.4434\n",
            "Epoch 3 Batch 1550 Loss 1.2859 Accuracy 0.4442\n",
            "Epoch 3 Batch 1600 Loss 1.2833 Accuracy 0.4451\n",
            "Epoch 3 Batch 1650 Loss 1.2807 Accuracy 0.4458\n",
            "Epoch 3 Batch 1700 Loss 1.2786 Accuracy 0.4464\n",
            "Epoch 3 Batch 1750 Loss 1.2759 Accuracy 0.4473\n",
            "Epoch 3 Batch 1800 Loss 1.2725 Accuracy 0.4482\n",
            "Epoch 3 Batch 1850 Loss 1.2694 Accuracy 0.4491\n",
            "Epoch 3 Batch 1900 Loss 1.2672 Accuracy 0.4498\n",
            "Epoch 3 Batch 1950 Loss 1.2649 Accuracy 0.4508\n",
            "Epoch 3 Batch 2000 Loss 1.2627 Accuracy 0.4516\n",
            "Epoch 3 Batch 2050 Loss 1.2602 Accuracy 0.4522\n",
            "Epoch 3 Batch 2100 Loss 1.2572 Accuracy 0.4526\n",
            "Epoch 3 Batch 2150 Loss 1.2539 Accuracy 0.4529\n",
            "Epoch 3 Batch 2200 Loss 1.2505 Accuracy 0.4530\n",
            "Epoch 3 Batch 2250 Loss 1.2470 Accuracy 0.4533\n",
            "Epoch 3 Batch 2300 Loss 1.2431 Accuracy 0.4535\n",
            "Epoch 3 Batch 2350 Loss 1.2393 Accuracy 0.4538\n",
            "Epoch 3 Batch 2400 Loss 1.2353 Accuracy 0.4541\n",
            "Epoch 3 Batch 2450 Loss 1.2320 Accuracy 0.4545\n",
            "Epoch 3 Batch 2500 Loss 1.2283 Accuracy 0.4550\n",
            "Epoch 3 Batch 2550 Loss 1.2251 Accuracy 0.4555\n",
            "Epoch 3 Batch 2600 Loss 1.2219 Accuracy 0.4559\n",
            "Epoch 3 Batch 2650 Loss 1.2191 Accuracy 0.4564\n",
            "Epoch 3 Batch 2700 Loss 1.2160 Accuracy 0.4569\n",
            "Epoch 3 Batch 2750 Loss 1.2129 Accuracy 0.4573\n",
            "Epoch 3 Batch 2800 Loss 1.2103 Accuracy 0.4576\n",
            "Epoch 3 Batch 2850 Loss 1.2076 Accuracy 0.4580\n",
            "Epoch 3 Batch 2900 Loss 1.2049 Accuracy 0.4583\n",
            "Epoch 3 Batch 2950 Loss 1.2027 Accuracy 0.4585\n",
            "Epoch 3 Batch 3000 Loss 1.2001 Accuracy 0.4588\n",
            "Epoch 3 Batch 3050 Loss 1.1979 Accuracy 0.4592\n",
            "Epoch 3 Batch 3100 Loss 1.1950 Accuracy 0.4595\n",
            "Epoch 3 Batch 3150 Loss 1.1926 Accuracy 0.4598\n",
            "Epoch 3 Batch 3200 Loss 1.1904 Accuracy 0.4602\n",
            "Epoch 3 Batch 3250 Loss 1.1879 Accuracy 0.4605\n",
            "Epoch 3 Batch 3300 Loss 1.1851 Accuracy 0.4608\n",
            "Epoch 3 Batch 3350 Loss 1.1826 Accuracy 0.4612\n",
            "Epoch 3 Batch 3400 Loss 1.1800 Accuracy 0.4616\n",
            "Epoch 3 Batch 3450 Loss 1.1779 Accuracy 0.4620\n",
            "Epoch 3 Batch 3500 Loss 1.1755 Accuracy 0.4623\n",
            "Epoch 3 Batch 3550 Loss 1.1732 Accuracy 0.4628\n",
            "Epoch 3 Batch 3600 Loss 1.1709 Accuracy 0.4631\n",
            "Epoch 3 Batch 3650 Loss 1.1687 Accuracy 0.4634\n",
            "Epoch 3 Batch 3700 Loss 1.1666 Accuracy 0.4639\n",
            "Epoch 3 Batch 3750 Loss 1.1643 Accuracy 0.4643\n",
            "Epoch 3 Batch 3800 Loss 1.1623 Accuracy 0.4648\n",
            "Epoch 3 Batch 3850 Loss 1.1604 Accuracy 0.4651\n",
            "Epoch 3 Batch 3900 Loss 1.1584 Accuracy 0.4655\n",
            "Epoch 3 Batch 3950 Loss 1.1568 Accuracy 0.4658\n",
            "Epoch 3 Batch 4000 Loss 1.1548 Accuracy 0.4661\n",
            "Epoch 3 Batch 4050 Loss 1.1530 Accuracy 0.4666\n",
            "Epoch 3 Batch 4100 Loss 1.1512 Accuracy 0.4668\n",
            "Epoch 3 Batch 4150 Loss 1.1503 Accuracy 0.4670\n",
            "Epoch 3 Batch 4200 Loss 1.1500 Accuracy 0.4672\n",
            "Epoch 3 Batch 4250 Loss 1.1498 Accuracy 0.4672\n",
            "Epoch 3 Batch 4300 Loss 1.1501 Accuracy 0.4672\n",
            "Epoch 3 Batch 4350 Loss 1.1508 Accuracy 0.4671\n",
            "Epoch 3 Batch 4400 Loss 1.1514 Accuracy 0.4671\n",
            "Epoch 3 Batch 4450 Loss 1.1522 Accuracy 0.4670\n",
            "Epoch 3 Batch 4500 Loss 1.1530 Accuracy 0.4669\n",
            "Epoch 3 Batch 4550 Loss 1.1542 Accuracy 0.4668\n",
            "Epoch 3 Batch 4600 Loss 1.1550 Accuracy 0.4667\n",
            "Epoch 3 Batch 4650 Loss 1.1559 Accuracy 0.4665\n",
            "Epoch 3 Batch 4700 Loss 1.1570 Accuracy 0.4663\n",
            "Epoch 3 Batch 4750 Loss 1.1580 Accuracy 0.4662\n",
            "Epoch 3 Batch 4800 Loss 1.1589 Accuracy 0.4661\n",
            "Epoch 3 Batch 4850 Loss 1.1600 Accuracy 0.4660\n",
            "Epoch 3 Batch 4900 Loss 1.1607 Accuracy 0.4659\n",
            "Epoch 3 Batch 4950 Loss 1.1613 Accuracy 0.4657\n",
            "Epoch 3 Batch 5000 Loss 1.1621 Accuracy 0.4656\n",
            "Epoch 3 Batch 5050 Loss 1.1630 Accuracy 0.4655\n",
            "Epoch 3 Batch 5100 Loss 1.1640 Accuracy 0.4653\n",
            "Epoch 3 Batch 5150 Loss 1.1651 Accuracy 0.4652\n",
            "Epoch 3 Batch 5200 Loss 1.1659 Accuracy 0.4650\n",
            "Epoch 3 Batch 5250 Loss 1.1668 Accuracy 0.4648\n",
            "Epoch 3 Batch 5300 Loss 1.1675 Accuracy 0.4645\n",
            "Epoch 3 Batch 5350 Loss 1.1681 Accuracy 0.4644\n",
            "Epoch 3 Batch 5400 Loss 1.1687 Accuracy 0.4642\n",
            "Epoch 3 Batch 5450 Loss 1.1694 Accuracy 0.4640\n",
            "Epoch 3 Batch 5500 Loss 1.1699 Accuracy 0.4638\n",
            "Epoch 3 Batch 5550 Loss 1.1706 Accuracy 0.4636\n",
            "Epoch 3 Batch 5600 Loss 1.1712 Accuracy 0.4634\n",
            "Epoch 3 Batch 5650 Loss 1.1716 Accuracy 0.4633\n",
            "Epoch 3 Batch 5700 Loss 1.1720 Accuracy 0.4631\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/transformers/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 1502.4797439575195 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.4006 Accuracy 0.4893\n",
            "Epoch 4 Batch 50 Loss 1.2449 Accuracy 0.4557\n",
            "Epoch 4 Batch 100 Loss 1.2411 Accuracy 0.4543\n",
            "Epoch 4 Batch 150 Loss 1.2344 Accuracy 0.4547\n",
            "Epoch 4 Batch 200 Loss 1.2303 Accuracy 0.4552\n",
            "Epoch 4 Batch 250 Loss 1.2261 Accuracy 0.4548\n",
            "Epoch 4 Batch 300 Loss 1.2212 Accuracy 0.4550\n",
            "Epoch 4 Batch 350 Loss 1.2231 Accuracy 0.4538\n",
            "Epoch 4 Batch 400 Loss 1.2217 Accuracy 0.4539\n",
            "Epoch 4 Batch 450 Loss 1.2150 Accuracy 0.4541\n",
            "Epoch 4 Batch 500 Loss 1.2144 Accuracy 0.4535\n",
            "Epoch 4 Batch 550 Loss 1.2135 Accuracy 0.4537\n",
            "Epoch 4 Batch 600 Loss 1.2116 Accuracy 0.4535\n",
            "Epoch 4 Batch 650 Loss 1.2101 Accuracy 0.4544\n",
            "Epoch 4 Batch 700 Loss 1.2070 Accuracy 0.4549\n",
            "Epoch 4 Batch 750 Loss 1.2053 Accuracy 0.4555\n",
            "Epoch 4 Batch 800 Loss 1.2036 Accuracy 0.4555\n",
            "Epoch 4 Batch 850 Loss 1.2048 Accuracy 0.4555\n",
            "Epoch 4 Batch 900 Loss 1.2020 Accuracy 0.4560\n",
            "Epoch 4 Batch 950 Loss 1.2002 Accuracy 0.4560\n",
            "Epoch 4 Batch 1000 Loss 1.1978 Accuracy 0.4560\n",
            "Epoch 4 Batch 1050 Loss 1.1969 Accuracy 0.4562\n",
            "Epoch 4 Batch 1100 Loss 1.1953 Accuracy 0.4564\n",
            "Epoch 4 Batch 1150 Loss 1.1931 Accuracy 0.4567\n",
            "Epoch 4 Batch 1200 Loss 1.1924 Accuracy 0.4568\n",
            "Epoch 4 Batch 1250 Loss 1.1902 Accuracy 0.4570\n",
            "Epoch 4 Batch 1300 Loss 1.1883 Accuracy 0.4575\n",
            "Epoch 4 Batch 1350 Loss 1.1860 Accuracy 0.4581\n",
            "Epoch 4 Batch 1400 Loss 1.1840 Accuracy 0.4588\n",
            "Epoch 4 Batch 1450 Loss 1.1819 Accuracy 0.4595\n",
            "Epoch 4 Batch 1500 Loss 1.1793 Accuracy 0.4604\n",
            "Epoch 4 Batch 1550 Loss 1.1770 Accuracy 0.4613\n",
            "Epoch 4 Batch 1600 Loss 1.1744 Accuracy 0.4622\n",
            "Epoch 4 Batch 1650 Loss 1.1718 Accuracy 0.4631\n",
            "Epoch 4 Batch 1700 Loss 1.1690 Accuracy 0.4640\n",
            "Epoch 4 Batch 1750 Loss 1.1665 Accuracy 0.4648\n",
            "Epoch 4 Batch 1800 Loss 1.1639 Accuracy 0.4655\n",
            "Epoch 4 Batch 1850 Loss 1.1611 Accuracy 0.4661\n",
            "Epoch 4 Batch 1900 Loss 1.1591 Accuracy 0.4669\n",
            "Epoch 4 Batch 1950 Loss 1.1563 Accuracy 0.4676\n",
            "Epoch 4 Batch 2000 Loss 1.1542 Accuracy 0.4683\n",
            "Epoch 4 Batch 2050 Loss 1.1516 Accuracy 0.4688\n",
            "Epoch 4 Batch 2100 Loss 1.1492 Accuracy 0.4692\n",
            "Epoch 4 Batch 2150 Loss 1.1463 Accuracy 0.4695\n",
            "Epoch 4 Batch 2200 Loss 1.1433 Accuracy 0.4697\n",
            "Epoch 4 Batch 2250 Loss 1.1396 Accuracy 0.4699\n",
            "Epoch 4 Batch 2300 Loss 1.1371 Accuracy 0.4701\n",
            "Epoch 4 Batch 2350 Loss 1.1343 Accuracy 0.4704\n",
            "Epoch 4 Batch 2400 Loss 1.1317 Accuracy 0.4707\n",
            "Epoch 4 Batch 2450 Loss 1.1287 Accuracy 0.4708\n",
            "Epoch 4 Batch 2500 Loss 1.1255 Accuracy 0.4712\n",
            "Epoch 4 Batch 2550 Loss 1.1216 Accuracy 0.4716\n",
            "Epoch 4 Batch 2600 Loss 1.1182 Accuracy 0.4718\n",
            "Epoch 4 Batch 2650 Loss 1.1151 Accuracy 0.4723\n",
            "Epoch 4 Batch 2700 Loss 1.1125 Accuracy 0.4727\n",
            "Epoch 4 Batch 2750 Loss 1.1098 Accuracy 0.4730\n",
            "Epoch 4 Batch 2800 Loss 1.1072 Accuracy 0.4734\n",
            "Epoch 4 Batch 2850 Loss 1.1053 Accuracy 0.4737\n",
            "Epoch 4 Batch 2900 Loss 1.1029 Accuracy 0.4740\n",
            "Epoch 4 Batch 2950 Loss 1.1006 Accuracy 0.4744\n",
            "Epoch 4 Batch 3000 Loss 1.0981 Accuracy 0.4748\n",
            "Epoch 4 Batch 3050 Loss 1.0960 Accuracy 0.4751\n",
            "Epoch 4 Batch 3100 Loss 1.0938 Accuracy 0.4755\n",
            "Epoch 4 Batch 3150 Loss 1.0915 Accuracy 0.4757\n",
            "Epoch 4 Batch 3200 Loss 1.0894 Accuracy 0.4759\n",
            "Epoch 4 Batch 3250 Loss 1.0872 Accuracy 0.4762\n",
            "Epoch 4 Batch 3300 Loss 1.0850 Accuracy 0.4765\n",
            "Epoch 4 Batch 3350 Loss 1.0828 Accuracy 0.4768\n",
            "Epoch 4 Batch 3400 Loss 1.0805 Accuracy 0.4771\n",
            "Epoch 4 Batch 3450 Loss 1.0790 Accuracy 0.4775\n",
            "Epoch 4 Batch 3500 Loss 1.0767 Accuracy 0.4779\n",
            "Epoch 4 Batch 3550 Loss 1.0749 Accuracy 0.4783\n",
            "Epoch 4 Batch 3600 Loss 1.0728 Accuracy 0.4785\n",
            "Epoch 4 Batch 3650 Loss 1.0706 Accuracy 0.4789\n",
            "Epoch 4 Batch 3700 Loss 1.0685 Accuracy 0.4793\n",
            "Epoch 4 Batch 3750 Loss 1.0669 Accuracy 0.4796\n",
            "Epoch 4 Batch 3800 Loss 1.0654 Accuracy 0.4799\n",
            "Epoch 4 Batch 3850 Loss 1.0637 Accuracy 0.4803\n",
            "Epoch 4 Batch 3900 Loss 1.0623 Accuracy 0.4806\n",
            "Epoch 4 Batch 3950 Loss 1.0605 Accuracy 0.4811\n",
            "Epoch 4 Batch 4000 Loss 1.0589 Accuracy 0.4815\n",
            "Epoch 4 Batch 4050 Loss 1.0578 Accuracy 0.4819\n",
            "Epoch 4 Batch 4100 Loss 1.0565 Accuracy 0.4822\n",
            "Epoch 4 Batch 4150 Loss 1.0560 Accuracy 0.4823\n",
            "Epoch 4 Batch 4200 Loss 1.0560 Accuracy 0.4823\n",
            "Epoch 4 Batch 4250 Loss 1.0560 Accuracy 0.4823\n",
            "Epoch 4 Batch 4300 Loss 1.0566 Accuracy 0.4822\n",
            "Epoch 4 Batch 4350 Loss 1.0575 Accuracy 0.4821\n",
            "Epoch 4 Batch 4400 Loss 1.0587 Accuracy 0.4820\n",
            "Epoch 4 Batch 4450 Loss 1.0596 Accuracy 0.4819\n",
            "Epoch 4 Batch 4500 Loss 1.0607 Accuracy 0.4817\n",
            "Epoch 4 Batch 4550 Loss 1.0618 Accuracy 0.4816\n",
            "Epoch 4 Batch 4600 Loss 1.0632 Accuracy 0.4814\n",
            "Epoch 4 Batch 4650 Loss 1.0642 Accuracy 0.4813\n",
            "Epoch 4 Batch 4700 Loss 1.0654 Accuracy 0.4811\n",
            "Epoch 4 Batch 4750 Loss 1.0665 Accuracy 0.4809\n",
            "Epoch 4 Batch 4800 Loss 1.0677 Accuracy 0.4808\n",
            "Epoch 4 Batch 4850 Loss 1.0687 Accuracy 0.4806\n",
            "Epoch 4 Batch 4900 Loss 1.0697 Accuracy 0.4805\n",
            "Epoch 4 Batch 4950 Loss 1.0707 Accuracy 0.4803\n",
            "Epoch 4 Batch 5000 Loss 1.0720 Accuracy 0.4801\n",
            "Epoch 4 Batch 5050 Loss 1.0728 Accuracy 0.4800\n",
            "Epoch 4 Batch 5100 Loss 1.0739 Accuracy 0.4797\n",
            "Epoch 4 Batch 5150 Loss 1.0750 Accuracy 0.4796\n",
            "Epoch 4 Batch 5200 Loss 1.0761 Accuracy 0.4793\n",
            "Epoch 4 Batch 5250 Loss 1.0770 Accuracy 0.4791\n",
            "Epoch 4 Batch 5300 Loss 1.0778 Accuracy 0.4788\n",
            "Epoch 4 Batch 5350 Loss 1.0785 Accuracy 0.4785\n",
            "Epoch 4 Batch 5400 Loss 1.0795 Accuracy 0.4783\n",
            "Epoch 4 Batch 5450 Loss 1.0803 Accuracy 0.4781\n",
            "Epoch 4 Batch 5500 Loss 1.0813 Accuracy 0.4779\n",
            "Epoch 4 Batch 5550 Loss 1.0821 Accuracy 0.4777\n",
            "Epoch 4 Batch 5600 Loss 1.0828 Accuracy 0.4774\n",
            "Epoch 4 Batch 5650 Loss 1.0836 Accuracy 0.4772\n",
            "Epoch 4 Batch 5700 Loss 1.0842 Accuracy 0.4771\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/transformers/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 1500.6318900585175 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.1725 Accuracy 0.4671\n",
            "Epoch 5 Batch 50 Loss 1.2000 Accuracy 0.4606\n",
            "Epoch 5 Batch 100 Loss 1.1833 Accuracy 0.4628\n",
            "Epoch 5 Batch 150 Loss 1.1626 Accuracy 0.4625\n",
            "Epoch 5 Batch 200 Loss 1.1594 Accuracy 0.4631\n",
            "Epoch 5 Batch 250 Loss 1.1561 Accuracy 0.4640\n",
            "Epoch 5 Batch 300 Loss 1.1581 Accuracy 0.4639\n",
            "Epoch 5 Batch 350 Loss 1.1567 Accuracy 0.4641\n",
            "Epoch 5 Batch 400 Loss 1.1534 Accuracy 0.4643\n",
            "Epoch 5 Batch 450 Loss 1.1544 Accuracy 0.4650\n",
            "Epoch 5 Batch 500 Loss 1.1522 Accuracy 0.4646\n",
            "Epoch 5 Batch 550 Loss 1.1476 Accuracy 0.4646\n",
            "Epoch 5 Batch 600 Loss 1.1472 Accuracy 0.4648\n",
            "Epoch 5 Batch 650 Loss 1.1456 Accuracy 0.4649\n",
            "Epoch 5 Batch 700 Loss 1.1452 Accuracy 0.4654\n",
            "Epoch 5 Batch 750 Loss 1.1431 Accuracy 0.4659\n",
            "Epoch 5 Batch 800 Loss 1.1425 Accuracy 0.4658\n",
            "Epoch 5 Batch 850 Loss 1.1413 Accuracy 0.4662\n",
            "Epoch 5 Batch 900 Loss 1.1415 Accuracy 0.4665\n",
            "Epoch 5 Batch 950 Loss 1.1395 Accuracy 0.4665\n",
            "Epoch 5 Batch 1000 Loss 1.1362 Accuracy 0.4667\n",
            "Epoch 5 Batch 1050 Loss 1.1346 Accuracy 0.4669\n",
            "Epoch 5 Batch 1100 Loss 1.1337 Accuracy 0.4669\n",
            "Epoch 5 Batch 1150 Loss 1.1334 Accuracy 0.4668\n",
            "Epoch 5 Batch 1200 Loss 1.1318 Accuracy 0.4670\n",
            "Epoch 5 Batch 1250 Loss 1.1301 Accuracy 0.4674\n",
            "Epoch 5 Batch 1300 Loss 1.1274 Accuracy 0.4678\n",
            "Epoch 5 Batch 1350 Loss 1.1253 Accuracy 0.4685\n",
            "Epoch 5 Batch 1400 Loss 1.1226 Accuracy 0.4691\n",
            "Epoch 5 Batch 1450 Loss 1.1207 Accuracy 0.4697\n",
            "Epoch 5 Batch 1500 Loss 1.1179 Accuracy 0.4707\n",
            "Epoch 5 Batch 1550 Loss 1.1153 Accuracy 0.4713\n",
            "Epoch 5 Batch 1600 Loss 1.1127 Accuracy 0.4721\n",
            "Epoch 5 Batch 1650 Loss 1.1096 Accuracy 0.4730\n",
            "Epoch 5 Batch 1700 Loss 1.1073 Accuracy 0.4739\n",
            "Epoch 5 Batch 1750 Loss 1.1046 Accuracy 0.4748\n",
            "Epoch 5 Batch 1800 Loss 1.1025 Accuracy 0.4755\n",
            "Epoch 5 Batch 1850 Loss 1.1009 Accuracy 0.4764\n",
            "Epoch 5 Batch 1900 Loss 1.0989 Accuracy 0.4772\n",
            "Epoch 5 Batch 1950 Loss 1.0969 Accuracy 0.4779\n",
            "Epoch 5 Batch 2000 Loss 1.0944 Accuracy 0.4785\n",
            "Epoch 5 Batch 2050 Loss 1.0923 Accuracy 0.4789\n",
            "Epoch 5 Batch 2100 Loss 1.0898 Accuracy 0.4793\n",
            "Epoch 5 Batch 2150 Loss 1.0872 Accuracy 0.4795\n",
            "Epoch 5 Batch 2200 Loss 1.0843 Accuracy 0.4797\n",
            "Epoch 5 Batch 2250 Loss 1.0809 Accuracy 0.4799\n",
            "Epoch 5 Batch 2300 Loss 1.0780 Accuracy 0.4802\n",
            "Epoch 5 Batch 2350 Loss 1.0750 Accuracy 0.4804\n",
            "Epoch 5 Batch 2400 Loss 1.0725 Accuracy 0.4807\n",
            "Epoch 5 Batch 2450 Loss 1.0693 Accuracy 0.4810\n",
            "Epoch 5 Batch 2500 Loss 1.0663 Accuracy 0.4812\n",
            "Epoch 5 Batch 2550 Loss 1.0631 Accuracy 0.4816\n",
            "Epoch 5 Batch 2600 Loss 1.0598 Accuracy 0.4820\n",
            "Epoch 5 Batch 2650 Loss 1.0569 Accuracy 0.4824\n",
            "Epoch 5 Batch 2700 Loss 1.0542 Accuracy 0.4827\n",
            "Epoch 5 Batch 2750 Loss 1.0514 Accuracy 0.4830\n",
            "Epoch 5 Batch 2800 Loss 1.0490 Accuracy 0.4833\n",
            "Epoch 5 Batch 2850 Loss 1.0469 Accuracy 0.4835\n",
            "Epoch 5 Batch 2900 Loss 1.0450 Accuracy 0.4839\n",
            "Epoch 5 Batch 2950 Loss 1.0425 Accuracy 0.4842\n",
            "Epoch 5 Batch 3000 Loss 1.0403 Accuracy 0.4845\n",
            "Epoch 5 Batch 3050 Loss 1.0380 Accuracy 0.4846\n",
            "Epoch 5 Batch 3100 Loss 1.0362 Accuracy 0.4850\n",
            "Epoch 5 Batch 3150 Loss 1.0341 Accuracy 0.4853\n",
            "Epoch 5 Batch 3200 Loss 1.0318 Accuracy 0.4855\n",
            "Epoch 5 Batch 3250 Loss 1.0298 Accuracy 0.4858\n",
            "Epoch 5 Batch 3300 Loss 1.0277 Accuracy 0.4862\n",
            "Epoch 5 Batch 3350 Loss 1.0258 Accuracy 0.4864\n",
            "Epoch 5 Batch 3400 Loss 1.0237 Accuracy 0.4866\n",
            "Epoch 5 Batch 3450 Loss 1.0217 Accuracy 0.4870\n",
            "Epoch 5 Batch 3500 Loss 1.0196 Accuracy 0.4874\n",
            "Epoch 5 Batch 3550 Loss 1.0179 Accuracy 0.4877\n",
            "Epoch 5 Batch 3600 Loss 1.0161 Accuracy 0.4880\n",
            "Epoch 5 Batch 3650 Loss 1.0143 Accuracy 0.4884\n",
            "Epoch 5 Batch 3700 Loss 1.0124 Accuracy 0.4887\n",
            "Epoch 5 Batch 3750 Loss 1.0111 Accuracy 0.4891\n",
            "Epoch 5 Batch 3800 Loss 1.0094 Accuracy 0.4895\n",
            "Epoch 5 Batch 3850 Loss 1.0080 Accuracy 0.4899\n",
            "Epoch 5 Batch 3900 Loss 1.0067 Accuracy 0.4901\n",
            "Epoch 5 Batch 3950 Loss 1.0053 Accuracy 0.4904\n",
            "Epoch 5 Batch 4000 Loss 1.0038 Accuracy 0.4907\n",
            "Epoch 5 Batch 4050 Loss 1.0022 Accuracy 0.4911\n",
            "Epoch 5 Batch 4100 Loss 1.0010 Accuracy 0.4913\n",
            "Epoch 5 Batch 4150 Loss 1.0006 Accuracy 0.4914\n",
            "Epoch 5 Batch 4200 Loss 1.0008 Accuracy 0.4914\n",
            "Epoch 5 Batch 4250 Loss 1.0012 Accuracy 0.4913\n",
            "Epoch 5 Batch 4300 Loss 1.0019 Accuracy 0.4912\n",
            "Epoch 5 Batch 4350 Loss 1.0029 Accuracy 0.4912\n",
            "Epoch 5 Batch 4400 Loss 1.0037 Accuracy 0.4911\n",
            "Epoch 5 Batch 4450 Loss 1.0048 Accuracy 0.4909\n",
            "Epoch 5 Batch 4500 Loss 1.0059 Accuracy 0.4907\n",
            "Epoch 5 Batch 4550 Loss 1.0071 Accuracy 0.4905\n",
            "Epoch 5 Batch 4600 Loss 1.0086 Accuracy 0.4904\n",
            "Epoch 5 Batch 4650 Loss 1.0100 Accuracy 0.4901\n",
            "Epoch 5 Batch 4700 Loss 1.0112 Accuracy 0.4899\n",
            "Epoch 5 Batch 4750 Loss 1.0125 Accuracy 0.4898\n",
            "Epoch 5 Batch 4800 Loss 1.0135 Accuracy 0.4896\n",
            "Epoch 5 Batch 4850 Loss 1.0146 Accuracy 0.4895\n",
            "Epoch 5 Batch 4900 Loss 1.0159 Accuracy 0.4893\n",
            "Epoch 5 Batch 4950 Loss 1.0173 Accuracy 0.4891\n",
            "Epoch 5 Batch 5000 Loss 1.0183 Accuracy 0.4890\n",
            "Epoch 5 Batch 5050 Loss 1.0197 Accuracy 0.4887\n",
            "Epoch 5 Batch 5100 Loss 1.0209 Accuracy 0.4885\n",
            "Epoch 5 Batch 5150 Loss 1.0219 Accuracy 0.4882\n",
            "Epoch 5 Batch 5200 Loss 1.0232 Accuracy 0.4879\n",
            "Epoch 5 Batch 5250 Loss 1.0241 Accuracy 0.4876\n",
            "Epoch 5 Batch 5300 Loss 1.0252 Accuracy 0.4874\n",
            "Epoch 5 Batch 5350 Loss 1.0260 Accuracy 0.4871\n",
            "Epoch 5 Batch 5400 Loss 1.0270 Accuracy 0.4869\n",
            "Epoch 5 Batch 5450 Loss 1.0279 Accuracy 0.4867\n",
            "Epoch 5 Batch 5500 Loss 1.0290 Accuracy 0.4864\n",
            "Epoch 5 Batch 5550 Loss 1.0300 Accuracy 0.4863\n",
            "Epoch 5 Batch 5600 Loss 1.0308 Accuracy 0.4861\n",
            "Epoch 5 Batch 5650 Loss 1.0313 Accuracy 0.4859\n",
            "Epoch 5 Batch 5700 Loss 1.0321 Accuracy 0.4856\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/transformers/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 1496.9396176338196 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.3152 Accuracy 0.4342\n",
            "Epoch 6 Batch 50 Loss 1.1493 Accuracy 0.4723\n",
            "Epoch 6 Batch 100 Loss 1.1339 Accuracy 0.4718\n",
            "Epoch 6 Batch 150 Loss 1.1242 Accuracy 0.4723\n",
            "Epoch 6 Batch 200 Loss 1.1134 Accuracy 0.4719\n",
            "Epoch 6 Batch 250 Loss 1.1133 Accuracy 0.4718\n",
            "Epoch 6 Batch 300 Loss 1.1163 Accuracy 0.4714\n",
            "Epoch 6 Batch 350 Loss 1.1145 Accuracy 0.4705\n",
            "Epoch 6 Batch 400 Loss 1.1112 Accuracy 0.4704\n",
            "Epoch 6 Batch 450 Loss 1.1105 Accuracy 0.4710\n",
            "Epoch 6 Batch 500 Loss 1.1089 Accuracy 0.4711\n",
            "Epoch 6 Batch 550 Loss 1.1069 Accuracy 0.4711\n",
            "Epoch 6 Batch 600 Loss 1.1073 Accuracy 0.4711\n",
            "Epoch 6 Batch 650 Loss 1.1055 Accuracy 0.4712\n",
            "Epoch 6 Batch 700 Loss 1.1056 Accuracy 0.4716\n",
            "Epoch 6 Batch 750 Loss 1.1063 Accuracy 0.4721\n",
            "Epoch 6 Batch 800 Loss 1.1059 Accuracy 0.4720\n",
            "Epoch 6 Batch 850 Loss 1.1043 Accuracy 0.4723\n",
            "Epoch 6 Batch 900 Loss 1.1023 Accuracy 0.4724\n",
            "Epoch 6 Batch 950 Loss 1.1007 Accuracy 0.4725\n",
            "Epoch 6 Batch 1000 Loss 1.0984 Accuracy 0.4725\n",
            "Epoch 6 Batch 1050 Loss 1.0966 Accuracy 0.4729\n",
            "Epoch 6 Batch 1100 Loss 1.0943 Accuracy 0.4730\n",
            "Epoch 6 Batch 1150 Loss 1.0927 Accuracy 0.4732\n",
            "Epoch 6 Batch 1200 Loss 1.0914 Accuracy 0.4736\n",
            "Epoch 6 Batch 1250 Loss 1.0894 Accuracy 0.4739\n",
            "Epoch 6 Batch 1300 Loss 1.0878 Accuracy 0.4743\n",
            "Epoch 6 Batch 1350 Loss 1.0850 Accuracy 0.4748\n",
            "Epoch 6 Batch 1400 Loss 1.0820 Accuracy 0.4753\n",
            "Epoch 6 Batch 1450 Loss 1.0793 Accuracy 0.4761\n",
            "Epoch 6 Batch 1500 Loss 1.0765 Accuracy 0.4769\n",
            "Epoch 6 Batch 1550 Loss 1.0741 Accuracy 0.4777\n",
            "Epoch 6 Batch 1600 Loss 1.0719 Accuracy 0.4785\n",
            "Epoch 6 Batch 1650 Loss 1.0694 Accuracy 0.4793\n",
            "Epoch 6 Batch 1700 Loss 1.0675 Accuracy 0.4803\n",
            "Epoch 6 Batch 1750 Loss 1.0652 Accuracy 0.4811\n",
            "Epoch 6 Batch 1800 Loss 1.0627 Accuracy 0.4820\n",
            "Epoch 6 Batch 1850 Loss 1.0606 Accuracy 0.4829\n",
            "Epoch 6 Batch 1900 Loss 1.0581 Accuracy 0.4838\n",
            "Epoch 6 Batch 1950 Loss 1.0555 Accuracy 0.4844\n",
            "Epoch 6 Batch 2000 Loss 1.0536 Accuracy 0.4851\n",
            "Epoch 6 Batch 2050 Loss 1.0514 Accuracy 0.4855\n",
            "Epoch 6 Batch 2100 Loss 1.0487 Accuracy 0.4859\n",
            "Epoch 6 Batch 2150 Loss 1.0459 Accuracy 0.4861\n",
            "Epoch 6 Batch 2200 Loss 1.0432 Accuracy 0.4863\n",
            "Epoch 6 Batch 2250 Loss 1.0402 Accuracy 0.4865\n",
            "Epoch 6 Batch 2300 Loss 1.0372 Accuracy 0.4867\n",
            "Epoch 6 Batch 2350 Loss 1.0350 Accuracy 0.4870\n",
            "Epoch 6 Batch 2400 Loss 1.0324 Accuracy 0.4873\n",
            "Epoch 6 Batch 2450 Loss 1.0287 Accuracy 0.4874\n",
            "Epoch 6 Batch 2500 Loss 1.0262 Accuracy 0.4877\n",
            "Epoch 6 Batch 2550 Loss 1.0230 Accuracy 0.4881\n",
            "Epoch 6 Batch 2600 Loss 1.0201 Accuracy 0.4884\n",
            "Epoch 6 Batch 2650 Loss 1.0173 Accuracy 0.4887\n",
            "Epoch 6 Batch 2700 Loss 1.0148 Accuracy 0.4890\n",
            "Epoch 6 Batch 2750 Loss 1.0127 Accuracy 0.4894\n",
            "Epoch 6 Batch 2800 Loss 1.0101 Accuracy 0.4897\n",
            "Epoch 6 Batch 2850 Loss 1.0080 Accuracy 0.4900\n",
            "Epoch 6 Batch 2900 Loss 1.0060 Accuracy 0.4903\n",
            "Epoch 6 Batch 2950 Loss 1.0038 Accuracy 0.4905\n",
            "Epoch 6 Batch 3000 Loss 1.0018 Accuracy 0.4907\n",
            "Epoch 6 Batch 3050 Loss 0.9999 Accuracy 0.4908\n",
            "Epoch 6 Batch 3100 Loss 0.9981 Accuracy 0.4911\n",
            "Epoch 6 Batch 3150 Loss 0.9960 Accuracy 0.4915\n",
            "Epoch 6 Batch 3200 Loss 0.9939 Accuracy 0.4918\n",
            "Epoch 6 Batch 3250 Loss 0.9917 Accuracy 0.4920\n",
            "Epoch 6 Batch 3300 Loss 0.9896 Accuracy 0.4923\n",
            "Epoch 6 Batch 3350 Loss 0.9878 Accuracy 0.4926\n",
            "Epoch 6 Batch 3400 Loss 0.9857 Accuracy 0.4929\n",
            "Epoch 6 Batch 3450 Loss 0.9840 Accuracy 0.4932\n",
            "Epoch 6 Batch 3500 Loss 0.9822 Accuracy 0.4935\n",
            "Epoch 6 Batch 3550 Loss 0.9804 Accuracy 0.4938\n",
            "Epoch 6 Batch 3600 Loss 0.9784 Accuracy 0.4941\n",
            "Epoch 6 Batch 3650 Loss 0.9769 Accuracy 0.4945\n",
            "Epoch 6 Batch 3700 Loss 0.9755 Accuracy 0.4948\n",
            "Epoch 6 Batch 3750 Loss 0.9740 Accuracy 0.4952\n",
            "Epoch 6 Batch 3800 Loss 0.9723 Accuracy 0.4954\n",
            "Epoch 6 Batch 3850 Loss 0.9704 Accuracy 0.4959\n",
            "Epoch 6 Batch 3900 Loss 0.9689 Accuracy 0.4962\n",
            "Epoch 6 Batch 3950 Loss 0.9677 Accuracy 0.4965\n",
            "Epoch 6 Batch 4000 Loss 0.9664 Accuracy 0.4968\n",
            "Epoch 6 Batch 4050 Loss 0.9651 Accuracy 0.4971\n",
            "Epoch 6 Batch 4100 Loss 0.9638 Accuracy 0.4973\n",
            "Epoch 6 Batch 4150 Loss 0.9633 Accuracy 0.4974\n",
            "Epoch 6 Batch 4200 Loss 0.9635 Accuracy 0.4974\n",
            "Epoch 6 Batch 4250 Loss 0.9639 Accuracy 0.4974\n",
            "Epoch 6 Batch 4300 Loss 0.9647 Accuracy 0.4973\n",
            "Epoch 6 Batch 4350 Loss 0.9658 Accuracy 0.4973\n",
            "Epoch 6 Batch 4400 Loss 0.9669 Accuracy 0.4971\n",
            "Epoch 6 Batch 4450 Loss 0.9680 Accuracy 0.4969\n",
            "Epoch 6 Batch 4500 Loss 0.9693 Accuracy 0.4967\n",
            "Epoch 6 Batch 4550 Loss 0.9704 Accuracy 0.4966\n",
            "Epoch 6 Batch 4600 Loss 0.9717 Accuracy 0.4964\n",
            "Epoch 6 Batch 4650 Loss 0.9731 Accuracy 0.4962\n",
            "Epoch 6 Batch 4700 Loss 0.9744 Accuracy 0.4960\n",
            "Epoch 6 Batch 4750 Loss 0.9758 Accuracy 0.4959\n",
            "Epoch 6 Batch 4800 Loss 0.9769 Accuracy 0.4957\n",
            "Epoch 6 Batch 4850 Loss 0.9783 Accuracy 0.4955\n",
            "Epoch 6 Batch 4900 Loss 0.9795 Accuracy 0.4954\n",
            "Epoch 6 Batch 4950 Loss 0.9806 Accuracy 0.4951\n",
            "Epoch 6 Batch 5000 Loss 0.9820 Accuracy 0.4949\n",
            "Epoch 6 Batch 5050 Loss 0.9832 Accuracy 0.4947\n",
            "Epoch 6 Batch 5100 Loss 0.9846 Accuracy 0.4945\n",
            "Epoch 6 Batch 5150 Loss 0.9857 Accuracy 0.4943\n",
            "Epoch 6 Batch 5200 Loss 0.9869 Accuracy 0.4941\n",
            "Epoch 6 Batch 5250 Loss 0.9880 Accuracy 0.4938\n",
            "Epoch 6 Batch 5300 Loss 0.9890 Accuracy 0.4935\n",
            "Epoch 6 Batch 5350 Loss 0.9900 Accuracy 0.4932\n",
            "Epoch 6 Batch 5400 Loss 0.9909 Accuracy 0.4930\n",
            "Epoch 6 Batch 5450 Loss 0.9921 Accuracy 0.4928\n",
            "Epoch 6 Batch 5500 Loss 0.9930 Accuracy 0.4925\n",
            "Epoch 6 Batch 5550 Loss 0.9941 Accuracy 0.4922\n",
            "Epoch 6 Batch 5600 Loss 0.9950 Accuracy 0.4920\n",
            "Epoch 6 Batch 5650 Loss 0.9958 Accuracy 0.4918\n",
            "Epoch 6 Batch 5700 Loss 0.9965 Accuracy 0.4915\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/transformers/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1499.0532054901123 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.0605 Accuracy 0.4449\n",
            "Epoch 7 Batch 50 Loss 1.0859 Accuracy 0.4744\n",
            "Epoch 7 Batch 100 Loss 1.0749 Accuracy 0.4754\n",
            "Epoch 7 Batch 150 Loss 1.0755 Accuracy 0.4748\n",
            "Epoch 7 Batch 200 Loss 1.0856 Accuracy 0.4754\n",
            "Epoch 7 Batch 250 Loss 1.0896 Accuracy 0.4756\n",
            "Epoch 7 Batch 300 Loss 1.0914 Accuracy 0.4754\n",
            "Epoch 7 Batch 350 Loss 1.0895 Accuracy 0.4758\n",
            "Epoch 7 Batch 400 Loss 1.0859 Accuracy 0.4759\n",
            "Epoch 7 Batch 450 Loss 1.0816 Accuracy 0.4755\n",
            "Epoch 7 Batch 500 Loss 1.0782 Accuracy 0.4754\n",
            "Epoch 7 Batch 550 Loss 1.0751 Accuracy 0.4756\n",
            "Epoch 7 Batch 600 Loss 1.0730 Accuracy 0.4756\n",
            "Epoch 7 Batch 650 Loss 1.0740 Accuracy 0.4760\n",
            "Epoch 7 Batch 700 Loss 1.0738 Accuracy 0.4759\n",
            "Epoch 7 Batch 750 Loss 1.0721 Accuracy 0.4766\n",
            "Epoch 7 Batch 800 Loss 1.0719 Accuracy 0.4769\n",
            "Epoch 7 Batch 850 Loss 1.0717 Accuracy 0.4770\n",
            "Epoch 7 Batch 900 Loss 1.0709 Accuracy 0.4768\n",
            "Epoch 7 Batch 950 Loss 1.0696 Accuracy 0.4772\n",
            "Epoch 7 Batch 1000 Loss 1.0671 Accuracy 0.4773\n",
            "Epoch 7 Batch 1050 Loss 1.0645 Accuracy 0.4774\n",
            "Epoch 7 Batch 1100 Loss 1.0631 Accuracy 0.4775\n",
            "Epoch 7 Batch 1150 Loss 1.0624 Accuracy 0.4776\n",
            "Epoch 7 Batch 1200 Loss 1.0624 Accuracy 0.4778\n",
            "Epoch 7 Batch 1250 Loss 1.0600 Accuracy 0.4781\n",
            "Epoch 7 Batch 1300 Loss 1.0580 Accuracy 0.4786\n",
            "Epoch 7 Batch 1350 Loss 1.0562 Accuracy 0.4791\n",
            "Epoch 7 Batch 1400 Loss 1.0540 Accuracy 0.4799\n",
            "Epoch 7 Batch 1450 Loss 1.0520 Accuracy 0.4806\n",
            "Epoch 7 Batch 1500 Loss 1.0494 Accuracy 0.4815\n",
            "Epoch 7 Batch 1550 Loss 1.0461 Accuracy 0.4825\n",
            "Epoch 7 Batch 1600 Loss 1.0432 Accuracy 0.4832\n",
            "Epoch 7 Batch 1650 Loss 1.0412 Accuracy 0.4839\n",
            "Epoch 7 Batch 1700 Loss 1.0387 Accuracy 0.4846\n",
            "Epoch 7 Batch 1750 Loss 1.0360 Accuracy 0.4855\n",
            "Epoch 7 Batch 1800 Loss 1.0337 Accuracy 0.4864\n",
            "Epoch 7 Batch 1850 Loss 1.0316 Accuracy 0.4872\n",
            "Epoch 7 Batch 1900 Loss 1.0291 Accuracy 0.4881\n",
            "Epoch 7 Batch 1950 Loss 1.0271 Accuracy 0.4889\n",
            "Epoch 7 Batch 2000 Loss 1.0251 Accuracy 0.4895\n",
            "Epoch 7 Batch 2050 Loss 1.0228 Accuracy 0.4899\n",
            "Epoch 7 Batch 2100 Loss 1.0202 Accuracy 0.4904\n",
            "Epoch 7 Batch 2150 Loss 1.0177 Accuracy 0.4906\n",
            "Epoch 7 Batch 2200 Loss 1.0148 Accuracy 0.4909\n",
            "Epoch 7 Batch 2250 Loss 1.0117 Accuracy 0.4912\n",
            "Epoch 7 Batch 2300 Loss 1.0088 Accuracy 0.4913\n",
            "Epoch 7 Batch 2350 Loss 1.0058 Accuracy 0.4916\n",
            "Epoch 7 Batch 2400 Loss 1.0027 Accuracy 0.4917\n",
            "Epoch 7 Batch 2450 Loss 0.9999 Accuracy 0.4920\n",
            "Epoch 7 Batch 2500 Loss 0.9970 Accuracy 0.4923\n",
            "Epoch 7 Batch 2550 Loss 0.9942 Accuracy 0.4926\n",
            "Epoch 7 Batch 2600 Loss 0.9916 Accuracy 0.4929\n",
            "Epoch 7 Batch 2650 Loss 0.9889 Accuracy 0.4933\n",
            "Epoch 7 Batch 2700 Loss 0.9863 Accuracy 0.4936\n",
            "Epoch 7 Batch 2750 Loss 0.9839 Accuracy 0.4939\n",
            "Epoch 7 Batch 2800 Loss 0.9817 Accuracy 0.4942\n",
            "Epoch 7 Batch 2850 Loss 0.9802 Accuracy 0.4945\n",
            "Epoch 7 Batch 2900 Loss 0.9780 Accuracy 0.4948\n",
            "Epoch 7 Batch 2950 Loss 0.9759 Accuracy 0.4951\n",
            "Epoch 7 Batch 3000 Loss 0.9742 Accuracy 0.4954\n",
            "Epoch 7 Batch 3050 Loss 0.9719 Accuracy 0.4957\n",
            "Epoch 7 Batch 3100 Loss 0.9701 Accuracy 0.4959\n",
            "Epoch 7 Batch 3150 Loss 0.9682 Accuracy 0.4962\n",
            "Epoch 7 Batch 3200 Loss 0.9662 Accuracy 0.4964\n",
            "Epoch 7 Batch 3250 Loss 0.9644 Accuracy 0.4967\n",
            "Epoch 7 Batch 3300 Loss 0.9627 Accuracy 0.4969\n",
            "Epoch 7 Batch 3350 Loss 0.9605 Accuracy 0.4972\n",
            "Epoch 7 Batch 3400 Loss 0.9586 Accuracy 0.4975\n",
            "Epoch 7 Batch 3450 Loss 0.9566 Accuracy 0.4978\n",
            "Epoch 7 Batch 3500 Loss 0.9548 Accuracy 0.4981\n",
            "Epoch 7 Batch 3550 Loss 0.9531 Accuracy 0.4985\n",
            "Epoch 7 Batch 3600 Loss 0.9511 Accuracy 0.4987\n",
            "Epoch 7 Batch 3650 Loss 0.9494 Accuracy 0.4990\n",
            "Epoch 7 Batch 3700 Loss 0.9477 Accuracy 0.4993\n",
            "Epoch 7 Batch 3750 Loss 0.9461 Accuracy 0.4996\n",
            "Epoch 7 Batch 3800 Loss 0.9446 Accuracy 0.4999\n",
            "Epoch 7 Batch 3850 Loss 0.9430 Accuracy 0.5003\n",
            "Epoch 7 Batch 3900 Loss 0.9415 Accuracy 0.5006\n",
            "Epoch 7 Batch 3950 Loss 0.9400 Accuracy 0.5009\n",
            "Epoch 7 Batch 4000 Loss 0.9385 Accuracy 0.5013\n",
            "Epoch 7 Batch 4050 Loss 0.9372 Accuracy 0.5016\n",
            "Epoch 7 Batch 4100 Loss 0.9363 Accuracy 0.5018\n",
            "Epoch 7 Batch 4150 Loss 0.9358 Accuracy 0.5019\n",
            "Epoch 7 Batch 4200 Loss 0.9359 Accuracy 0.5019\n",
            "Epoch 7 Batch 4250 Loss 0.9363 Accuracy 0.5019\n",
            "Epoch 7 Batch 4300 Loss 0.9373 Accuracy 0.5018\n",
            "Epoch 7 Batch 4350 Loss 0.9379 Accuracy 0.5017\n",
            "Epoch 7 Batch 4400 Loss 0.9391 Accuracy 0.5015\n",
            "Epoch 7 Batch 4450 Loss 0.9402 Accuracy 0.5013\n",
            "Epoch 7 Batch 4500 Loss 0.9416 Accuracy 0.5011\n",
            "Epoch 7 Batch 4550 Loss 0.9428 Accuracy 0.5009\n",
            "Epoch 7 Batch 4600 Loss 0.9443 Accuracy 0.5008\n",
            "Epoch 7 Batch 4650 Loss 0.9456 Accuracy 0.5006\n",
            "Epoch 7 Batch 4700 Loss 0.9470 Accuracy 0.5004\n",
            "Epoch 7 Batch 4750 Loss 0.9481 Accuracy 0.5003\n",
            "Epoch 7 Batch 4800 Loss 0.9494 Accuracy 0.5001\n",
            "Epoch 7 Batch 4850 Loss 0.9509 Accuracy 0.4999\n",
            "Epoch 7 Batch 4900 Loss 0.9521 Accuracy 0.4997\n",
            "Epoch 7 Batch 4950 Loss 0.9534 Accuracy 0.4995\n",
            "Epoch 7 Batch 5000 Loss 0.9548 Accuracy 0.4993\n",
            "Epoch 7 Batch 5050 Loss 0.9560 Accuracy 0.4991\n",
            "Epoch 7 Batch 5100 Loss 0.9574 Accuracy 0.4988\n",
            "Epoch 7 Batch 5150 Loss 0.9585 Accuracy 0.4986\n",
            "Epoch 7 Batch 5200 Loss 0.9598 Accuracy 0.4984\n",
            "Epoch 7 Batch 5250 Loss 0.9611 Accuracy 0.4981\n",
            "Epoch 7 Batch 5300 Loss 0.9622 Accuracy 0.4978\n",
            "Epoch 7 Batch 5350 Loss 0.9634 Accuracy 0.4976\n",
            "Epoch 7 Batch 5400 Loss 0.9645 Accuracy 0.4973\n",
            "Epoch 7 Batch 5450 Loss 0.9655 Accuracy 0.4970\n",
            "Epoch 7 Batch 5500 Loss 0.9665 Accuracy 0.4967\n",
            "Epoch 7 Batch 5550 Loss 0.9673 Accuracy 0.4964\n",
            "Epoch 7 Batch 5600 Loss 0.9681 Accuracy 0.4962\n",
            "Epoch 7 Batch 5650 Loss 0.9690 Accuracy 0.4959\n",
            "Epoch 7 Batch 5700 Loss 0.9699 Accuracy 0.4957\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/transformers/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 1496.081073999405 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.1049 Accuracy 0.4433\n",
            "Epoch 8 Batch 50 Loss 1.0763 Accuracy 0.4794\n",
            "Epoch 8 Batch 100 Loss 1.0688 Accuracy 0.4801\n",
            "Epoch 8 Batch 150 Loss 1.0614 Accuracy 0.4783\n",
            "Epoch 8 Batch 200 Loss 1.0660 Accuracy 0.4788\n",
            "Epoch 8 Batch 250 Loss 1.0655 Accuracy 0.4785\n",
            "Epoch 8 Batch 300 Loss 1.0633 Accuracy 0.4789\n",
            "Epoch 8 Batch 350 Loss 1.0622 Accuracy 0.4796\n",
            "Epoch 8 Batch 400 Loss 1.0608 Accuracy 0.4800\n",
            "Epoch 8 Batch 450 Loss 1.0568 Accuracy 0.4800\n",
            "Epoch 8 Batch 500 Loss 1.0534 Accuracy 0.4797\n",
            "Epoch 8 Batch 550 Loss 1.0538 Accuracy 0.4796\n",
            "Epoch 8 Batch 600 Loss 1.0533 Accuracy 0.4795\n",
            "Epoch 8 Batch 650 Loss 1.0516 Accuracy 0.4799\n",
            "Epoch 8 Batch 700 Loss 1.0511 Accuracy 0.4803\n",
            "Epoch 8 Batch 750 Loss 1.0506 Accuracy 0.4804\n",
            "Epoch 8 Batch 800 Loss 1.0477 Accuracy 0.4803\n",
            "Epoch 8 Batch 850 Loss 1.0469 Accuracy 0.4805\n",
            "Epoch 8 Batch 900 Loss 1.0476 Accuracy 0.4810\n",
            "Epoch 8 Batch 950 Loss 1.0460 Accuracy 0.4809\n",
            "Epoch 8 Batch 1000 Loss 1.0441 Accuracy 0.4813\n",
            "Epoch 8 Batch 1050 Loss 1.0433 Accuracy 0.4813\n",
            "Epoch 8 Batch 1100 Loss 1.0422 Accuracy 0.4814\n",
            "Epoch 8 Batch 1150 Loss 1.0403 Accuracy 0.4815\n",
            "Epoch 8 Batch 1200 Loss 1.0384 Accuracy 0.4817\n",
            "Epoch 8 Batch 1250 Loss 1.0366 Accuracy 0.4820\n",
            "Epoch 8 Batch 1300 Loss 1.0354 Accuracy 0.4826\n",
            "Epoch 8 Batch 1350 Loss 1.0325 Accuracy 0.4831\n",
            "Epoch 8 Batch 1400 Loss 1.0293 Accuracy 0.4839\n",
            "Epoch 8 Batch 1450 Loss 1.0270 Accuracy 0.4848\n",
            "Epoch 8 Batch 1500 Loss 1.0244 Accuracy 0.4856\n",
            "Epoch 8 Batch 1550 Loss 1.0223 Accuracy 0.4863\n",
            "Epoch 8 Batch 1600 Loss 1.0200 Accuracy 0.4872\n",
            "Epoch 8 Batch 1650 Loss 1.0175 Accuracy 0.4881\n",
            "Epoch 8 Batch 1700 Loss 1.0151 Accuracy 0.4889\n",
            "Epoch 8 Batch 1750 Loss 1.0128 Accuracy 0.4897\n",
            "Epoch 8 Batch 1800 Loss 1.0109 Accuracy 0.4906\n",
            "Epoch 8 Batch 1850 Loss 1.0080 Accuracy 0.4914\n",
            "Epoch 8 Batch 1900 Loss 1.0060 Accuracy 0.4922\n",
            "Epoch 8 Batch 1950 Loss 1.0039 Accuracy 0.4928\n",
            "Epoch 8 Batch 2000 Loss 1.0014 Accuracy 0.4935\n",
            "Epoch 8 Batch 2050 Loss 0.9989 Accuracy 0.4940\n",
            "Epoch 8 Batch 2100 Loss 0.9963 Accuracy 0.4943\n",
            "Epoch 8 Batch 2150 Loss 0.9936 Accuracy 0.4945\n",
            "Epoch 8 Batch 2200 Loss 0.9904 Accuracy 0.4946\n",
            "Epoch 8 Batch 2250 Loss 0.9873 Accuracy 0.4949\n",
            "Epoch 8 Batch 2300 Loss 0.9848 Accuracy 0.4951\n",
            "Epoch 8 Batch 2350 Loss 0.9819 Accuracy 0.4954\n",
            "Epoch 8 Batch 2400 Loss 0.9789 Accuracy 0.4956\n",
            "Epoch 8 Batch 2450 Loss 0.9767 Accuracy 0.4959\n",
            "Epoch 8 Batch 2500 Loss 0.9735 Accuracy 0.4963\n",
            "Epoch 8 Batch 2550 Loss 0.9708 Accuracy 0.4967\n",
            "Epoch 8 Batch 2600 Loss 0.9681 Accuracy 0.4971\n",
            "Epoch 8 Batch 2650 Loss 0.9655 Accuracy 0.4973\n",
            "Epoch 8 Batch 2700 Loss 0.9628 Accuracy 0.4976\n",
            "Epoch 8 Batch 2750 Loss 0.9606 Accuracy 0.4980\n",
            "Epoch 8 Batch 2800 Loss 0.9579 Accuracy 0.4982\n",
            "Epoch 8 Batch 2850 Loss 0.9562 Accuracy 0.4985\n",
            "Epoch 8 Batch 2900 Loss 0.9547 Accuracy 0.4987\n",
            "Epoch 8 Batch 2950 Loss 0.9527 Accuracy 0.4990\n",
            "Epoch 8 Batch 3000 Loss 0.9510 Accuracy 0.4993\n",
            "Epoch 8 Batch 3050 Loss 0.9492 Accuracy 0.4996\n",
            "Epoch 8 Batch 3100 Loss 0.9472 Accuracy 0.4998\n",
            "Epoch 8 Batch 3150 Loss 0.9454 Accuracy 0.5001\n",
            "Epoch 8 Batch 3200 Loss 0.9433 Accuracy 0.5003\n",
            "Epoch 8 Batch 3250 Loss 0.9412 Accuracy 0.5006\n",
            "Epoch 8 Batch 3300 Loss 0.9391 Accuracy 0.5009\n",
            "Epoch 8 Batch 3350 Loss 0.9370 Accuracy 0.5011\n",
            "Epoch 8 Batch 3400 Loss 0.9351 Accuracy 0.5014\n",
            "Epoch 8 Batch 3450 Loss 0.9330 Accuracy 0.5016\n",
            "Epoch 8 Batch 3500 Loss 0.9313 Accuracy 0.5020\n",
            "Epoch 8 Batch 3550 Loss 0.9296 Accuracy 0.5023\n",
            "Epoch 8 Batch 3600 Loss 0.9282 Accuracy 0.5026\n",
            "Epoch 8 Batch 3650 Loss 0.9266 Accuracy 0.5028\n",
            "Epoch 8 Batch 3700 Loss 0.9251 Accuracy 0.5032\n",
            "Epoch 8 Batch 3750 Loss 0.9236 Accuracy 0.5035\n",
            "Epoch 8 Batch 3800 Loss 0.9220 Accuracy 0.5038\n",
            "Epoch 8 Batch 3850 Loss 0.9207 Accuracy 0.5041\n",
            "Epoch 8 Batch 3900 Loss 0.9193 Accuracy 0.5044\n",
            "Epoch 8 Batch 3950 Loss 0.9179 Accuracy 0.5047\n",
            "Epoch 8 Batch 4000 Loss 0.9165 Accuracy 0.5050\n",
            "Epoch 8 Batch 4050 Loss 0.9152 Accuracy 0.5054\n",
            "Epoch 8 Batch 4100 Loss 0.9144 Accuracy 0.5055\n",
            "Epoch 8 Batch 4150 Loss 0.9141 Accuracy 0.5056\n",
            "Epoch 8 Batch 4200 Loss 0.9142 Accuracy 0.5056\n",
            "Epoch 8 Batch 4250 Loss 0.9147 Accuracy 0.5055\n",
            "Epoch 8 Batch 4300 Loss 0.9157 Accuracy 0.5054\n",
            "Epoch 8 Batch 4350 Loss 0.9166 Accuracy 0.5053\n",
            "Epoch 8 Batch 4400 Loss 0.9176 Accuracy 0.5052\n",
            "Epoch 8 Batch 4450 Loss 0.9187 Accuracy 0.5050\n",
            "Epoch 8 Batch 4500 Loss 0.9202 Accuracy 0.5048\n",
            "Epoch 8 Batch 4550 Loss 0.9216 Accuracy 0.5047\n",
            "Epoch 8 Batch 4600 Loss 0.9229 Accuracy 0.5044\n",
            "Epoch 8 Batch 4650 Loss 0.9244 Accuracy 0.5042\n",
            "Epoch 8 Batch 4700 Loss 0.9257 Accuracy 0.5040\n",
            "Epoch 8 Batch 4750 Loss 0.9268 Accuracy 0.5039\n",
            "Epoch 8 Batch 4800 Loss 0.9278 Accuracy 0.5037\n",
            "Epoch 8 Batch 4850 Loss 0.9290 Accuracy 0.5035\n",
            "Epoch 8 Batch 4900 Loss 0.9302 Accuracy 0.5033\n",
            "Epoch 8 Batch 4950 Loss 0.9314 Accuracy 0.5032\n",
            "Epoch 8 Batch 5000 Loss 0.9329 Accuracy 0.5030\n",
            "Epoch 8 Batch 5050 Loss 0.9340 Accuracy 0.5028\n",
            "Epoch 8 Batch 5100 Loss 0.9351 Accuracy 0.5026\n",
            "Epoch 8 Batch 5150 Loss 0.9367 Accuracy 0.5023\n",
            "Epoch 8 Batch 5200 Loss 0.9381 Accuracy 0.5020\n",
            "Epoch 8 Batch 5250 Loss 0.9392 Accuracy 0.5018\n",
            "Epoch 8 Batch 5300 Loss 0.9404 Accuracy 0.5014\n",
            "Epoch 8 Batch 5350 Loss 0.9414 Accuracy 0.5012\n",
            "Epoch 8 Batch 5400 Loss 0.9426 Accuracy 0.5009\n",
            "Epoch 8 Batch 5450 Loss 0.9437 Accuracy 0.5006\n",
            "Epoch 8 Batch 5500 Loss 0.9446 Accuracy 0.5004\n",
            "Epoch 8 Batch 5550 Loss 0.9457 Accuracy 0.5002\n",
            "Epoch 8 Batch 5600 Loss 0.9466 Accuracy 0.4999\n",
            "Epoch 8 Batch 5650 Loss 0.9476 Accuracy 0.4997\n",
            "Epoch 8 Batch 5700 Loss 0.9484 Accuracy 0.4994\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/transformers/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 1498.0939433574677 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.9928 Accuracy 0.4433\n",
            "Epoch 9 Batch 50 Loss 1.0742 Accuracy 0.4820\n",
            "Epoch 9 Batch 100 Loss 1.0634 Accuracy 0.4832\n",
            "Epoch 9 Batch 150 Loss 1.0443 Accuracy 0.4837\n",
            "Epoch 9 Batch 200 Loss 1.0415 Accuracy 0.4838\n",
            "Epoch 9 Batch 250 Loss 1.0414 Accuracy 0.4835\n",
            "Epoch 9 Batch 300 Loss 1.0415 Accuracy 0.4834\n",
            "Epoch 9 Batch 350 Loss 1.0440 Accuracy 0.4836\n",
            "Epoch 9 Batch 400 Loss 1.0404 Accuracy 0.4842\n",
            "Epoch 9 Batch 450 Loss 1.0396 Accuracy 0.4840\n",
            "Epoch 9 Batch 500 Loss 1.0347 Accuracy 0.4835\n",
            "Epoch 9 Batch 550 Loss 1.0331 Accuracy 0.4833\n",
            "Epoch 9 Batch 600 Loss 1.0328 Accuracy 0.4839\n",
            "Epoch 9 Batch 650 Loss 1.0315 Accuracy 0.4837\n",
            "Epoch 9 Batch 700 Loss 1.0301 Accuracy 0.4842\n",
            "Epoch 9 Batch 750 Loss 1.0292 Accuracy 0.4843\n",
            "Epoch 9 Batch 800 Loss 1.0278 Accuracy 0.4844\n",
            "Epoch 9 Batch 850 Loss 1.0276 Accuracy 0.4845\n",
            "Epoch 9 Batch 900 Loss 1.0262 Accuracy 0.4848\n",
            "Epoch 9 Batch 950 Loss 1.0251 Accuracy 0.4850\n",
            "Epoch 9 Batch 1000 Loss 1.0231 Accuracy 0.4849\n",
            "Epoch 9 Batch 1050 Loss 1.0223 Accuracy 0.4847\n",
            "Epoch 9 Batch 1100 Loss 1.0209 Accuracy 0.4850\n",
            "Epoch 9 Batch 1150 Loss 1.0195 Accuracy 0.4852\n",
            "Epoch 9 Batch 1200 Loss 1.0181 Accuracy 0.4854\n",
            "Epoch 9 Batch 1250 Loss 1.0161 Accuracy 0.4858\n",
            "Epoch 9 Batch 1300 Loss 1.0140 Accuracy 0.4865\n",
            "Epoch 9 Batch 1350 Loss 1.0114 Accuracy 0.4870\n",
            "Epoch 9 Batch 1400 Loss 1.0090 Accuracy 0.4876\n",
            "Epoch 9 Batch 1450 Loss 1.0072 Accuracy 0.4882\n",
            "Epoch 9 Batch 1500 Loss 1.0039 Accuracy 0.4890\n",
            "Epoch 9 Batch 1550 Loss 1.0017 Accuracy 0.4899\n",
            "Epoch 9 Batch 1600 Loss 0.9992 Accuracy 0.4908\n",
            "Epoch 9 Batch 1650 Loss 0.9973 Accuracy 0.4916\n",
            "Epoch 9 Batch 1700 Loss 0.9944 Accuracy 0.4922\n",
            "Epoch 9 Batch 1750 Loss 0.9922 Accuracy 0.4930\n",
            "Epoch 9 Batch 1800 Loss 0.9901 Accuracy 0.4939\n",
            "Epoch 9 Batch 1850 Loss 0.9878 Accuracy 0.4947\n",
            "Epoch 9 Batch 1900 Loss 0.9854 Accuracy 0.4954\n",
            "Epoch 9 Batch 1950 Loss 0.9831 Accuracy 0.4961\n",
            "Epoch 9 Batch 2000 Loss 0.9812 Accuracy 0.4967\n",
            "Epoch 9 Batch 2050 Loss 0.9793 Accuracy 0.4973\n",
            "Epoch 9 Batch 2100 Loss 0.9772 Accuracy 0.4977\n",
            "Epoch 9 Batch 2150 Loss 0.9746 Accuracy 0.4979\n",
            "Epoch 9 Batch 2200 Loss 0.9719 Accuracy 0.4982\n",
            "Epoch 9 Batch 2250 Loss 0.9685 Accuracy 0.4984\n",
            "Epoch 9 Batch 2300 Loss 0.9656 Accuracy 0.4985\n",
            "Epoch 9 Batch 2350 Loss 0.9634 Accuracy 0.4987\n",
            "Epoch 9 Batch 2400 Loss 0.9608 Accuracy 0.4990\n",
            "Epoch 9 Batch 2450 Loss 0.9582 Accuracy 0.4992\n",
            "Epoch 9 Batch 2500 Loss 0.9551 Accuracy 0.4996\n",
            "Epoch 9 Batch 2550 Loss 0.9521 Accuracy 0.4997\n",
            "Epoch 9 Batch 2600 Loss 0.9493 Accuracy 0.5001\n",
            "Epoch 9 Batch 2650 Loss 0.9466 Accuracy 0.5004\n",
            "Epoch 9 Batch 2700 Loss 0.9441 Accuracy 0.5008\n",
            "Epoch 9 Batch 2750 Loss 0.9419 Accuracy 0.5010\n",
            "Epoch 9 Batch 2800 Loss 0.9398 Accuracy 0.5013\n",
            "Epoch 9 Batch 2850 Loss 0.9378 Accuracy 0.5016\n",
            "Epoch 9 Batch 2900 Loss 0.9357 Accuracy 0.5018\n",
            "Epoch 9 Batch 2950 Loss 0.9338 Accuracy 0.5021\n",
            "Epoch 9 Batch 3000 Loss 0.9319 Accuracy 0.5023\n",
            "Epoch 9 Batch 3050 Loss 0.9302 Accuracy 0.5025\n",
            "Epoch 9 Batch 3100 Loss 0.9283 Accuracy 0.5027\n",
            "Epoch 9 Batch 3150 Loss 0.9266 Accuracy 0.5029\n",
            "Epoch 9 Batch 3200 Loss 0.9243 Accuracy 0.5031\n",
            "Epoch 9 Batch 3250 Loss 0.9224 Accuracy 0.5033\n",
            "Epoch 9 Batch 3300 Loss 0.9205 Accuracy 0.5036\n",
            "Epoch 9 Batch 3350 Loss 0.9187 Accuracy 0.5039\n",
            "Epoch 9 Batch 3400 Loss 0.9166 Accuracy 0.5043\n",
            "Epoch 9 Batch 3450 Loss 0.9148 Accuracy 0.5046\n",
            "Epoch 9 Batch 3500 Loss 0.9131 Accuracy 0.5049\n",
            "Epoch 9 Batch 3550 Loss 0.9115 Accuracy 0.5053\n",
            "Epoch 9 Batch 3600 Loss 0.9094 Accuracy 0.5055\n",
            "Epoch 9 Batch 3650 Loss 0.9075 Accuracy 0.5059\n",
            "Epoch 9 Batch 3700 Loss 0.9061 Accuracy 0.5062\n",
            "Epoch 9 Batch 3750 Loss 0.9046 Accuracy 0.5065\n",
            "Epoch 9 Batch 3800 Loss 0.9032 Accuracy 0.5068\n",
            "Epoch 9 Batch 3850 Loss 0.9021 Accuracy 0.5072\n",
            "Epoch 9 Batch 3900 Loss 0.9008 Accuracy 0.5074\n",
            "Epoch 9 Batch 3950 Loss 0.8995 Accuracy 0.5077\n",
            "Epoch 9 Batch 4000 Loss 0.8982 Accuracy 0.5080\n",
            "Epoch 9 Batch 4050 Loss 0.8969 Accuracy 0.5084\n",
            "Epoch 9 Batch 4100 Loss 0.8961 Accuracy 0.5086\n",
            "Epoch 9 Batch 4150 Loss 0.8957 Accuracy 0.5086\n",
            "Epoch 9 Batch 4200 Loss 0.8958 Accuracy 0.5086\n",
            "Epoch 9 Batch 4250 Loss 0.8961 Accuracy 0.5085\n",
            "Epoch 9 Batch 4300 Loss 0.8971 Accuracy 0.5085\n",
            "Epoch 9 Batch 4350 Loss 0.8981 Accuracy 0.5084\n",
            "Epoch 9 Batch 4400 Loss 0.8991 Accuracy 0.5082\n",
            "Epoch 9 Batch 4450 Loss 0.9001 Accuracy 0.5081\n",
            "Epoch 9 Batch 4500 Loss 0.9015 Accuracy 0.5079\n",
            "Epoch 9 Batch 4550 Loss 0.9032 Accuracy 0.5077\n",
            "Epoch 9 Batch 4600 Loss 0.9047 Accuracy 0.5075\n",
            "Epoch 9 Batch 4650 Loss 0.9060 Accuracy 0.5073\n",
            "Epoch 9 Batch 4700 Loss 0.9071 Accuracy 0.5072\n",
            "Epoch 9 Batch 4750 Loss 0.9083 Accuracy 0.5070\n",
            "Epoch 9 Batch 4800 Loss 0.9094 Accuracy 0.5068\n",
            "Epoch 9 Batch 4850 Loss 0.9108 Accuracy 0.5066\n",
            "Epoch 9 Batch 4900 Loss 0.9119 Accuracy 0.5064\n",
            "Epoch 9 Batch 4950 Loss 0.9133 Accuracy 0.5062\n",
            "Epoch 9 Batch 5000 Loss 0.9146 Accuracy 0.5060\n",
            "Epoch 9 Batch 5050 Loss 0.9161 Accuracy 0.5058\n",
            "Epoch 9 Batch 5100 Loss 0.9173 Accuracy 0.5056\n",
            "Epoch 9 Batch 5150 Loss 0.9187 Accuracy 0.5053\n",
            "Epoch 9 Batch 5200 Loss 0.9200 Accuracy 0.5050\n",
            "Epoch 9 Batch 5250 Loss 0.9211 Accuracy 0.5048\n",
            "Epoch 9 Batch 5300 Loss 0.9225 Accuracy 0.5045\n",
            "Epoch 9 Batch 5350 Loss 0.9235 Accuracy 0.5042\n",
            "Epoch 9 Batch 5400 Loss 0.9249 Accuracy 0.5040\n",
            "Epoch 9 Batch 5450 Loss 0.9259 Accuracy 0.5037\n",
            "Epoch 9 Batch 5500 Loss 0.9270 Accuracy 0.5034\n",
            "Epoch 9 Batch 5550 Loss 0.9280 Accuracy 0.5032\n",
            "Epoch 9 Batch 5600 Loss 0.9288 Accuracy 0.5029\n",
            "Epoch 9 Batch 5650 Loss 0.9299 Accuracy 0.5026\n",
            "Epoch 9 Batch 5700 Loss 0.9306 Accuracy 0.5024\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/transformers/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 1497.3505930900574 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.1394 Accuracy 0.4836\n",
            "Epoch 10 Batch 50 Loss 1.0428 Accuracy 0.4845\n",
            "Epoch 10 Batch 100 Loss 1.0341 Accuracy 0.4845\n",
            "Epoch 10 Batch 150 Loss 1.0283 Accuracy 0.4846\n",
            "Epoch 10 Batch 200 Loss 1.0236 Accuracy 0.4856\n",
            "Epoch 10 Batch 250 Loss 1.0255 Accuracy 0.4864\n",
            "Epoch 10 Batch 300 Loss 1.0247 Accuracy 0.4862\n",
            "Epoch 10 Batch 350 Loss 1.0238 Accuracy 0.4860\n",
            "Epoch 10 Batch 400 Loss 1.0226 Accuracy 0.4858\n",
            "Epoch 10 Batch 450 Loss 1.0172 Accuracy 0.4855\n",
            "Epoch 10 Batch 500 Loss 1.0156 Accuracy 0.4859\n",
            "Epoch 10 Batch 550 Loss 1.0138 Accuracy 0.4863\n",
            "Epoch 10 Batch 600 Loss 1.0141 Accuracy 0.4862\n",
            "Epoch 10 Batch 650 Loss 1.0142 Accuracy 0.4861\n",
            "Epoch 10 Batch 700 Loss 1.0132 Accuracy 0.4864\n",
            "Epoch 10 Batch 750 Loss 1.0128 Accuracy 0.4865\n",
            "Epoch 10 Batch 800 Loss 1.0129 Accuracy 0.4868\n",
            "Epoch 10 Batch 850 Loss 1.0129 Accuracy 0.4870\n",
            "Epoch 10 Batch 900 Loss 1.0123 Accuracy 0.4871\n",
            "Epoch 10 Batch 950 Loss 1.0087 Accuracy 0.4872\n",
            "Epoch 10 Batch 1000 Loss 1.0072 Accuracy 0.4876\n",
            "Epoch 10 Batch 1050 Loss 1.0063 Accuracy 0.4875\n",
            "Epoch 10 Batch 1100 Loss 1.0060 Accuracy 0.4874\n",
            "Epoch 10 Batch 1150 Loss 1.0054 Accuracy 0.4877\n",
            "Epoch 10 Batch 1200 Loss 1.0033 Accuracy 0.4878\n",
            "Epoch 10 Batch 1250 Loss 1.0009 Accuracy 0.4885\n",
            "Epoch 10 Batch 1300 Loss 0.9982 Accuracy 0.4889\n",
            "Epoch 10 Batch 1350 Loss 0.9959 Accuracy 0.4895\n",
            "Epoch 10 Batch 1400 Loss 0.9938 Accuracy 0.4902\n",
            "Epoch 10 Batch 1450 Loss 0.9912 Accuracy 0.4909\n",
            "Epoch 10 Batch 1500 Loss 0.9886 Accuracy 0.4918\n",
            "Epoch 10 Batch 1550 Loss 0.9859 Accuracy 0.4925\n",
            "Epoch 10 Batch 1600 Loss 0.9838 Accuracy 0.4932\n",
            "Epoch 10 Batch 1650 Loss 0.9813 Accuracy 0.4940\n",
            "Epoch 10 Batch 1700 Loss 0.9787 Accuracy 0.4947\n",
            "Epoch 10 Batch 1750 Loss 0.9762 Accuracy 0.4954\n",
            "Epoch 10 Batch 1800 Loss 0.9739 Accuracy 0.4962\n",
            "Epoch 10 Batch 1850 Loss 0.9713 Accuracy 0.4970\n",
            "Epoch 10 Batch 1900 Loss 0.9689 Accuracy 0.4977\n",
            "Epoch 10 Batch 1950 Loss 0.9667 Accuracy 0.4984\n",
            "Epoch 10 Batch 2000 Loss 0.9652 Accuracy 0.4990\n",
            "Epoch 10 Batch 2050 Loss 0.9629 Accuracy 0.4996\n",
            "Epoch 10 Batch 2100 Loss 0.9604 Accuracy 0.4998\n",
            "Epoch 10 Batch 2150 Loss 0.9578 Accuracy 0.5001\n",
            "Epoch 10 Batch 2200 Loss 0.9552 Accuracy 0.5003\n",
            "Epoch 10 Batch 2250 Loss 0.9524 Accuracy 0.5006\n",
            "Epoch 10 Batch 2300 Loss 0.9493 Accuracy 0.5008\n",
            "Epoch 10 Batch 2350 Loss 0.9463 Accuracy 0.5011\n",
            "Epoch 10 Batch 2400 Loss 0.9438 Accuracy 0.5013\n",
            "Epoch 10 Batch 2450 Loss 0.9409 Accuracy 0.5016\n",
            "Epoch 10 Batch 2500 Loss 0.9379 Accuracy 0.5019\n",
            "Epoch 10 Batch 2550 Loss 0.9355 Accuracy 0.5022\n",
            "Epoch 10 Batch 2600 Loss 0.9332 Accuracy 0.5024\n",
            "Epoch 10 Batch 2650 Loss 0.9301 Accuracy 0.5029\n",
            "Epoch 10 Batch 2700 Loss 0.9276 Accuracy 0.5031\n",
            "Epoch 10 Batch 2750 Loss 0.9254 Accuracy 0.5034\n",
            "Epoch 10 Batch 2800 Loss 0.9235 Accuracy 0.5038\n",
            "Epoch 10 Batch 2850 Loss 0.9211 Accuracy 0.5041\n",
            "Epoch 10 Batch 2900 Loss 0.9192 Accuracy 0.5044\n",
            "Epoch 10 Batch 2950 Loss 0.9173 Accuracy 0.5046\n",
            "Epoch 10 Batch 3000 Loss 0.9154 Accuracy 0.5049\n",
            "Epoch 10 Batch 3050 Loss 0.9137 Accuracy 0.5051\n",
            "Epoch 10 Batch 3100 Loss 0.9123 Accuracy 0.5053\n",
            "Epoch 10 Batch 3150 Loss 0.9105 Accuracy 0.5056\n",
            "Epoch 10 Batch 3200 Loss 0.9088 Accuracy 0.5057\n",
            "Epoch 10 Batch 3250 Loss 0.9067 Accuracy 0.5060\n",
            "Epoch 10 Batch 3300 Loss 0.9048 Accuracy 0.5062\n",
            "Epoch 10 Batch 3350 Loss 0.9032 Accuracy 0.5065\n",
            "Epoch 10 Batch 3400 Loss 0.9011 Accuracy 0.5068\n",
            "Epoch 10 Batch 3450 Loss 0.8993 Accuracy 0.5072\n",
            "Epoch 10 Batch 3500 Loss 0.8979 Accuracy 0.5075\n",
            "Epoch 10 Batch 3550 Loss 0.8964 Accuracy 0.5078\n",
            "Epoch 10 Batch 3600 Loss 0.8948 Accuracy 0.5082\n",
            "Epoch 10 Batch 3650 Loss 0.8931 Accuracy 0.5085\n",
            "Epoch 10 Batch 3700 Loss 0.8915 Accuracy 0.5088\n",
            "Epoch 10 Batch 3750 Loss 0.8899 Accuracy 0.5090\n",
            "Epoch 10 Batch 3800 Loss 0.8886 Accuracy 0.5094\n",
            "Epoch 10 Batch 3850 Loss 0.8871 Accuracy 0.5097\n",
            "Epoch 10 Batch 3900 Loss 0.8857 Accuracy 0.5100\n",
            "Epoch 10 Batch 3950 Loss 0.8844 Accuracy 0.5104\n",
            "Epoch 10 Batch 4000 Loss 0.8833 Accuracy 0.5106\n",
            "Epoch 10 Batch 4050 Loss 0.8820 Accuracy 0.5109\n",
            "Epoch 10 Batch 4100 Loss 0.8811 Accuracy 0.5111\n",
            "Epoch 10 Batch 4150 Loss 0.8807 Accuracy 0.5112\n",
            "Epoch 10 Batch 4200 Loss 0.8813 Accuracy 0.5112\n",
            "Epoch 10 Batch 4250 Loss 0.8816 Accuracy 0.5112\n",
            "Epoch 10 Batch 4300 Loss 0.8821 Accuracy 0.5112\n",
            "Epoch 10 Batch 4350 Loss 0.8829 Accuracy 0.5110\n",
            "Epoch 10 Batch 4400 Loss 0.8840 Accuracy 0.5108\n",
            "Epoch 10 Batch 4450 Loss 0.8853 Accuracy 0.5107\n",
            "Epoch 10 Batch 4500 Loss 0.8866 Accuracy 0.5105\n",
            "Epoch 10 Batch 4550 Loss 0.8881 Accuracy 0.5103\n",
            "Epoch 10 Batch 4600 Loss 0.8896 Accuracy 0.5101\n",
            "Epoch 10 Batch 4650 Loss 0.8907 Accuracy 0.5099\n",
            "Epoch 10 Batch 4700 Loss 0.8920 Accuracy 0.5096\n",
            "Epoch 10 Batch 4750 Loss 0.8933 Accuracy 0.5094\n",
            "Epoch 10 Batch 4800 Loss 0.8947 Accuracy 0.5092\n",
            "Epoch 10 Batch 4850 Loss 0.8960 Accuracy 0.5090\n",
            "Epoch 10 Batch 4900 Loss 0.8971 Accuracy 0.5089\n",
            "Epoch 10 Batch 4950 Loss 0.8986 Accuracy 0.5086\n",
            "Epoch 10 Batch 5000 Loss 0.8999 Accuracy 0.5084\n",
            "Epoch 10 Batch 5050 Loss 0.9014 Accuracy 0.5082\n",
            "Epoch 10 Batch 5100 Loss 0.9026 Accuracy 0.5079\n",
            "Epoch 10 Batch 5150 Loss 0.9037 Accuracy 0.5077\n",
            "Epoch 10 Batch 5200 Loss 0.9050 Accuracy 0.5074\n",
            "Epoch 10 Batch 5250 Loss 0.9063 Accuracy 0.5071\n",
            "Epoch 10 Batch 5300 Loss 0.9075 Accuracy 0.5068\n",
            "Epoch 10 Batch 5350 Loss 0.9087 Accuracy 0.5065\n",
            "Epoch 10 Batch 5400 Loss 0.9098 Accuracy 0.5062\n",
            "Epoch 10 Batch 5450 Loss 0.9110 Accuracy 0.5059\n",
            "Epoch 10 Batch 5500 Loss 0.9120 Accuracy 0.5057\n",
            "Epoch 10 Batch 5550 Loss 0.9129 Accuracy 0.5054\n",
            "Epoch 10 Batch 5600 Loss 0.9137 Accuracy 0.5052\n",
            "Epoch 10 Batch 5650 Loss 0.9146 Accuracy 0.5050\n",
            "Epoch 10 Batch 5700 Loss 0.9155 Accuracy 0.5047\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/transformers/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1503.001368522644 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egrsNGQU0Bx6"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ljg30wI0DKu"
      },
      "source": [
        "def evaluate(inp_sentence):\r\n",
        "    inp_sentence = \\\r\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\r\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\r\n",
        "    \r\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\r\n",
        "    \r\n",
        "    for _ in range(MAX_LENGTH):\r\n",
        "        predictions = transformer(enc_input, output, False)\r\n",
        "        \r\n",
        "        prediction = predictions[:, -1:, :]\r\n",
        "        \r\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\r\n",
        "        \r\n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\r\n",
        "            return tf.squeeze(output, axis=0)\r\n",
        "        \r\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\r\n",
        "        \r\n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0rVKQMq1La3"
      },
      "source": [
        "def translate(sentence):\r\n",
        "    output = evaluate(sentence).numpy()\r\n",
        "    \r\n",
        "    predicted_sentence = tokenizer_fr.decode(\r\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\r\n",
        "    )\r\n",
        "    \r\n",
        "    print(\"Input: {}\".format(sentence))\r\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaWCS70L1i1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6552b75-efdc-4c1f-c886-03e825d6c1a7"
      },
      "source": [
        "translate(\"This is my first model.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is my first model.\n",
            "Predicted translation: C'est mon premier modle.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkSPoZuX2WKV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}